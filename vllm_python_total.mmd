classDiagram
    %% 엔진 및 스케줄러
    class LLMEngine {
        +vllm_config: VllmConfig
        +model_executor: ExecutorBase
        +scheduler: list[Scheduler]
        +output_processor: SequenceGroupOutputProcessor
        +tokenizer: Optional[TokenizerGroup]
        +seq_id_to_seq_group: Dict[str, SequenceGroupBase]
        +add_request(...)
        +step()
        +abort_request(...)
        +add_lora(...)
        +remove_lora(...)
        +list_loras()
        +pin_lora(...)
        +start_profile()
        +stop_profile()
        +sleep()
        +wake_up()
        +is_sleeping()
        +check_health()
        +reset_mm_cache()
        +reset_prefix_cache()
    }

    class AsyncLLMEngine {
        +engine: _AsyncLLMEngine
        +request_tracker: RequestTracker
        +background_loop: Optional[asyncio.Future]
        +add_request(...)
        +generate(...)
        +encode(...)
        +abort(...)
        +start_background_loop()
        +shutdown_background_loop()
    }
    EngineClient <|-- AsyncLLMEngine

    class _AsyncLLMEngine {
        +step_async(...)
        +add_request_async(...)
        +check_health_async()
        +collective_rpc_async(...)
    }
    LLMEngine <|-- _AsyncLLMEngine

    class Scheduler {
        +waiting: Deque[SequenceGroup]
        +running: Deque[SequenceGroup]
        +swapped: Deque[SequenceGroup]
        +block_manager: BlockSpaceManager
        +add_seq_group(...)
        +schedule()
        +fork_seq(...)
        +free_seq(...)
        +remove_seq_from_computed_blocks_tracker(...)
        +free_finished_seq_groups()
        +reset_prefix_cache()
        +get_prefix_cache_hit_rate()
    }

    class SchedulerOutputs {
        +scheduled_seq_groups: Sequence[ScheduledSequenceGroup]
        +blocks_to_swap_in: List[Tuple[int, int]]
        +blocks_to_swap_out: List[Tuple[int, int]]
        +blocks_to_copy: List[Tuple[int, int]]
        +ignored_seq_groups: List[SequenceGroup]
        +num_lookahead_slots: int
        +running_queue_size: int
        +preempted: int
    }

    class ScheduledSequenceGroup {
        +seq_group: SequenceGroup
        +token_chunk_size: int
    }

    class SequenceGroupOutputProcessor {
        <<interface>>
        +process_outputs(sequence_group, outputs, is_async)
        +process_prompt_logprob(seq_group, outputs)
        +create_output_processor(...)
    }

    class SingleStepOutputProcessor {
        +scheduler_config: SchedulerConfig
        +detokenizer: Detokenizer
        +scheduler: List[Scheduler]
        +seq_counter: Counter
        +stop_checker: StopChecker
        +process_outputs(...)
        +process_prompt_logprob(...)
    }
    SequenceGroupOutputProcessor <|-- SingleStepOutputProcessor

    %% 블록 관리
    class BlockSpaceManager {
        <<interface>>
        +can_allocate(...)
        +allocate(...)
        +free(...)
        +fork(...)
        +swap_in(...)
        +swap_out(...)
        +reset_prefix_cache(...)
        +get_prefix_cache_hit_rate(...)
    }

    class SelfAttnBlockSpaceManager {
        +block_allocator: CpuGpuBlockAllocator
        +block_tables: Dict[SeqId, BlockTable]
        +cross_block_tables: Dict[EncoderSeqId, BlockTable]
        +_computed_blocks_tracker: ComputedBlocksTracker
        +_last_access_blocks_tracker: LastAccessBlocksTracker
        +allocate(...)
        +free(...)
        +fork(...)
        +swap_in(...)
        +swap_out(...)
        +can_allocate(...)
        +can_append_slots(...)
        +append_slots(...)
        +get_block_table(...)
        +get_cross_block_table(...)
        +access_all_blocks_in_seq(...)
        +mark_blocks_as_computed(...)
        +get_common_computed_block_ids(...)
    }
    BlockSpaceManager <|-- SelfAttnBlockSpaceManager

    class CpuGpuBlockAllocator {
        +_allocators: Dict[Device, BlockAllocator]
        +_swap_mapping: Dict[int, int]
        +_null_block: Optional[Block]
        +allocate_mutable_block(...)
        +allocate_immutable_block(...)
        +free(...)
        +fork(...)
        +swap(...)
        +get_num_free_blocks(...)
        +get_num_total_blocks(...)
        +get_physical_block_id(...)
        +clear_copy_on_writes(...)
        +mark_blocks_as_accessed(...)
        +mark_blocks_as_computed(...)
        +get_common_computed_block_ids(...)
        +reset_prefix_cache(...)
        +get_prefix_cache_hit_rate(...)
    }

    class BlockAllocator {
        <<interface>>
        +allocate_mutable_block(...)
        +allocate_immutable_block(...)
        +free(...)
        +fork(...)
        +swap_in(...)
        +swap_out(...)
        +get_num_free_blocks(...)
        +get_num_total_blocks(...)
        +get_physical_block_id(...)
        +reset_prefix_cache(...)
        +get_prefix_cache_hit_rate(...)
    }

    class NaiveBlockAllocator
    BlockAllocator <|-- NaiveBlockAllocator

    class PrefixCachingBlockAllocator
    BlockAllocator <|-- PrefixCachingBlockAllocator

    class BlockTable {
        +block_size: int
        +block_allocator: CpuGpuBlockAllocator
        +blocks: List[Block]
        +allocate(...)
        +fork(...)
        +update(...)
        +free(...)
        +get_num_blocks_touched_by_append_slots(...)
        +get_unseen_token_ids(...)
        +append_token_ids(...)
        +physical_block_ids: List[int]
    }

    class Block {
        +block_id: int
        +token_ids: List[int]
        +num_tokens_total: int
        +num_empty_slots: int
        +is_full: bool
        +prev_block: Optional[Block]
        +extra_hash: Optional[int]
        +computed: bool
        +last_accessed: float
        +content_hash: Optional[int]
        +append_token_ids(...)
    }

    class NullBlock {
        +_proxy: Block
        +append_token_ids(...)
        +block_id
        +token_ids
        +num_tokens_total
        +num_empty_slots
        +is_full
        +prev_block
        +extra_hash
        +computed
        +last_accessed
        +content_hash
    }
    Block <|-- NullBlock

    %% 모델 로더
    class BaseModelLoader {
        +load_config: LoadConfig
        +download_model(...)
        +load_weights(...)
        +load_model(...)
    }

    class DefaultModelLoader {
        +Source
        +counter_before_loading_weights: float
        +counter_after_loading_weights: float
        +_prepare_weights(...)
        +_get_weights_iterator(...)
        +get_all_weights(...)
        +download_model(...)
        +load_weights(...)
    }
    BaseModelLoader <|-- DefaultModelLoader

    class GGUFModelLoader {
        +_prepare_weights(...)
        +_get_gguf_weights_map(...)
        +_get_weights_iterator(...)
        +download_model(...)
        +load_weights(...)
        +load_model(...)
    }
    BaseModelLoader <|-- GGUFModelLoader

    %% 레이어 및 모델
    class FusedMoEMethodBase {
        <<abstract>>
        +moe: FusedMoEConfig
        +create_weights(...)
        +uses_weight_scale_2_pattern()
        +maybe_make_prepare_finalize(...)
        +init_prepare_finalize(...)
        +select_gemm_impl(...)
        +maybe_swap_experts_impl(...)
        +apply(...)
    }
    QuantizeMethodBase <|-- FusedMoEMethodBase

    class UnquantizedFusedMoEMethod {
        +select_gemm_impl(...)
        +create_weights(...)
        +_maybe_pad_weight(...)
        +process_weights_after_loading(...)
        +apply(...)
        +forward_cuda(...)
        +forward_cpu(...)
        +forward_xpu(...)
        +forward_tpu(...)
    }
    FusedMoEMethodBase <|-- UnquantizedFusedMoEMethod
    CustomOp <|-- UnquantizedFusedMoEMethod

    class FusedMoE {
        +moe_parallel_config: FusedMoEParallelConfig
        +global_num_experts: int
        +local_num_experts: int
        +quant_method: FusedMoEMethodBase
        +forward(...)
        +forward_impl_chunked(...)
        +forward_impl(...)
        +weight_loader(...)
        +get_expert_weights(...)
        +set_eplb_state(...)
        +must_reduce_shared_expert_outputs(...)
        +maybe_all_reduce_tensor_model_parallel(...)
    }
    torch.nn.Module <|-- FusedMoE

    class LinearMethodBase {
        <<abstract>>
        +create_weights(...)
        +apply(...)
    }
    QuantizeMethodBase <|-- LinearMethodBase

    class UnquantizedLinearMethod {
        +create_weights(...)
        +process_weights_after_loading(...)
        +apply(...)
    }
    LinearMethodBase <|-- UnquantizedLinearMethod

    class LinearBase {
        +input_size: int
        +output_size: int
        +skip_bias_add: bool
        +params_dtype: Optional[torch.dtype]
        +quant_config: Optional[QuantizationConfig]
        +prefix: str
        +return_bias: bool
        +forward(...)
    }
    torch.nn.Module <|-- LinearBase

    class ReplicatedLinear {
        +output_partition_sizes: list[int]
        +quant_method: LinearMethodBase
        +weight_loader(...)
        +forward(...)
    }
    LinearBase <|-- ReplicatedLinear

    class MergedReplicatedLinear {
        +output_sizes: list[int]
        +weight_loader(...)
        +forward(...)
    }
    ReplicatedLinear <|-- MergedReplicatedLinear

    class ColumnParallelLinear {
        +weight_loader(...)
        +weight_loader_v2(...)
        +forward(...)
    }
    LinearBase <|-- ColumnParallelLinear

    class MergedColumnParallelLinear {
        +output_sizes: list[int]
        +weight_loader(...)
        +weight_loader_v2(...)
        +forward(...)
    }
    ColumnParallelLinear <|-- MergedColumnParallelLinear

    class QKVParallelLinear {
        +_get_shard_offset_mapping(...)
        +_get_shard_size_mapping(...)
        +weight_loader(...)
        +weight_loader_v2(...)
        +forward(...)
    }
    ColumnParallelLinear <|-- QKVParallelLinear

    class RowParallelLinear {
        +weight_loader(...)
        +weight_loader_v2(...)
        +forward(...)
    }
    LinearBase <|-- RowParallelLinear

    class QKVCrossParallelLinear {
        +sync_weight_attrs(...)
        +select_proj_params(...)
        +forward(...)
        +weight_loader(...)
    }
    LinearBase <|-- QKVCrossParallelLinear

    %% 워커 및 분산 실행자
    class WorkerBase {
        +init_device()
        +initialize_cache(...)
        +get_model()
        +load_model()
        +execute_model(...)
        +add_lora(...)
        +remove_lora(...)
        +pin_lora(...)
        +list_loras()
    }

    class LocalOrDistributedWorkerBase {
        +do_metadata_broadcast: bool
        +kv_cache
        +prepare_worker_input(...)
        +execute_worker(...)
        +_get_worker_input_from_broadcast()
        +_get_driver_input_and_broadcast(...)
        +prepare_input(...)
        +get_model()
        +execute_model(...)
        +_execute_model_spmd(...)
    }
    WorkerBase <|-- LocalOrDistributedWorkerBase

    class Worker {
        +model_runner: GPUModelRunnerBase
        +cache_engine: List[CacheEngine]
        +gpu_cache: Optional[List[List[torch.Tensor]]]
        +_seq_group_metadata_cache: Dict[str, SequenceGroupMetadata]
        +add_lora(...)
        +remove_lora(...)
        +pin_lora(...)
        +list_loras()
        +max_model_len
        +vocab_size
        +get_cache_block_size_bytes()
        +init_device()
        +load_model()
        +save_sharded_state(...)
        +save_tensorized_model(...)
        +determine_num_available_blocks()
        +initialize_cache(...)
        +prepare_worker_input(...)
        +execute_worker(...)
        +_get_cached_seq_group_metadata(...)
        +_execute_model_spmd(...)
    }
    LocalOrDistributedWorkerBase <|-- Worker

    class RayDistributedExecutor {
        +supports_pp: bool
        +_init_executor()
        +max_concurrent_batches
        +execute_model(...)
        +reinitialize_distributed(...)
        +kv_output_aggregator: KVOutputAggregator
    }
    RayDistributedExecutorV0 <|-- RayDistributedExecutor
    Executor <|-- RayDistributedExecutor

    class FutureWrapper {
        +refs
        +aggregator: Optional[KVOutputAggregator]
        +result(timeout)
    }
    Future <|-- FutureWrapper

    class MultiprocExecutor {
        +supports_pp: bool
        +_init_executor()
        +start_worker_monitor()
        +register_failure_callback(...)
        +execute_model(...)
        +collective_rpc(...)
        +shutdown()
        +check_health()
        +max_concurrent_batches
        +_get_output_rank()
        +workers: list[WorkerProcHandle]
        +rpc_broadcast_mq: MessageQueue
        +io_thread_pool: Optional[ThreadPoolExecutor]
        +failure_callback: Optional[FailureCallback]
        +kv_output_aggregator: KVOutputAggregator
    }
    Executor <|-- MultiprocExecutor

    class WorkerProc {
        +READY_STR
        +rpc_broadcast_mq: MessageQueue
        +worker_response_mq: MessageQueue
        +worker: WorkerWrapperBase
        +make_worker_process(...)
        +wait_for_ready(...)
        +shutdown()
        +worker_main(...)
        +worker_busy_loop()
        +ResponseStatus(Enum)
    }

    class WorkerProcHandle {
        +proc: BaseProcess
        +rank: int
        +worker_response_mq: MessageQueue
        +death_writer: Optional[Connection]
        +from_unready_handle(...)
    }

    class UnreadyWorkerProcHandle {
        +proc: BaseProcess
        +rank: int
        +ready_pipe: Connection
        +death_writer: Optional[Connection]
    }

    class WorkerWrapperBase {
        +rpc_rank
        +worker: Optional[WorkerBase]
        +vllm_config: Optional[VllmConfig]
        +adjust_rank(...)
        +update_environment_variables(...)
        +init_worker(...)
        +initialize_from_config(...)
        +init_device()
        +execute_method(...)
        +__getattr__(...)
    }

    %% 기타 유틸리티 및 엔트리포인트
    class OpenAIServing {
        +engine_client: EngineClient
        +model_config: ModelConfig
        +models: OpenAIServingModels
        +request_logger: Optional[RequestLogger]
        +_preprocess(...)
        +_build_response(...)
        +handle(...)
        +_pipeline(...)
        +_validate_request(...)
        +_create_pooling_params(...)
        +_prepare_generators(...)
        +_collect_batch(...)
        +create_error_response(...)
        +create_streaming_error_response(...)
        +_check_model(...)
        +_maybe_get_adapters(...)
        +_get_message_types(...)
        +_normalize_prompt_text_to_input(...)
        +_normalize_prompt_tokens_to_input(...)
        +_validate_input(...)
        +_tokenize_prompt_input_async(...)
        +_tokenize_prompt_inputs_async(...)
        +_tokenize_prompt_input_or_inputs_async(...)
        +_preprocess_completion(...)
        +_preprocess_chat(...)
        +_generate_with_builtin_tools(...)
        +_load_prompt_embeds(...)
        +_log_inputs(...)
        +_get_trace_headers(...)
        +_is_model_supported(...)
        +_get_model_name(...)
    }

    class EmbeddingMixin {
        +_preprocess(...)
        +_build_response(...)
    }
    OpenAIServing <|-- EmbeddingMixin

    class OpenAIServingEmbedding {
        +request_id_prefix: str
        +create_embedding(...)
        +_validate_request(...)
        +_create_pooling_params(...)
    }
    EmbeddingMixin <|-- OpenAIServingEmbedding

    %% 관계 및 타입 연결
    LLMEngine --> Scheduler : manages
    LLMEngine --> SequenceGroupOutputProcessor : uses
    LLMEngine --> ExecutorBase : uses
    LLMEngine --> TokenizerGroup : uses
    LLMEngine --> SequenceGroupBase : manages
    LLMEngine --> StatLoggerBase : logs
    LLMEngine --> StopChecker : uses
    LLMEngine --> Detokenizer : uses
    LLMEngine --> LoRARequest : manages
    LLMEngine --> RequestOutput : returns
    LLMEngine --> PoolingRequestOutput : returns

    Scheduler --> BlockSpaceManager : manages
    Scheduler --> SequenceGroup : schedules
    Scheduler --> ScheduledSequenceGroup : uses
    Scheduler --> SchedulerOutputs : returns

    BlockSpaceManager --> CpuGpuBlockAllocator : uses
    CpuGpuBlockAllocator --> BlockAllocator : uses
    BlockAllocator --> Block : allocates
    BlockTable --> Block : manages
    BlockTable --> CpuGpuBlockAllocator : uses
    BlockTable --> BlockTable : forks

    Worker --> GPUModelRunnerBase : uses
    Worker --> CacheEngine : uses
    Worker --> SequenceGroupMetadata : manages
    Worker --> LoRARequest : manages
    Worker --> SamplerOutput : returns

    MultiprocExecutor --> WorkerProc : manages
    MultiprocExecutor --> WorkerProcHandle : manages
    MultiprocExecutor --> UnreadyWorkerProcHandle : manages

    RayDistributedExecutor --> KVOutputAggregator : uses
    RayDistributedExecutor --> FutureWrapper : returns

    OpenAIServingEmbedding --> EmbeddingServeContext : uses
    OpenAIServingEmbedding --> EmbeddingRequest : uses
    OpenAIServingEmbedding --> EmbeddingResponse : returns

    %% 기타 타입 연결
    SingleStepOutputProcessor --> Scheduler : uses
    SingleStepOutputProcessor --> Detokenizer : uses
    SingleStepOutputProcessor --> Counter : uses
    SingleStepOutputProcessor --> StopChecker : uses

    %% Enum
    class WorkerProc.ResponseStatus {
        SUCCESS
        FAILURE
    }
    class PreemptionMode {
        SWAP
        RECOMPUTE
    }