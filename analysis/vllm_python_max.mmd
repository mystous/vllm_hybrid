classDiagram
%% ========================= Utils =========================
class ConstantList {
  +__init__(x: list[T])
  +append(item)
  +extend(item)
  +insert(item)
  +pop(item)
  +remove(item)
  +clear()
  +index(item, start, stop)
  +__getitem__(item)
  +__setitem__(item, value)
  +__delitem__(item)
  +__iter__()
  +__contains__(item)
  +__len__()
  +__repr__()
}

class APIServerProcessManager {
  +__init__(target_server_fn, listen_address, sock, args, num_servers, input_addresses, output_addresses, stats_update_address)
  +close()
}

%% ========================= Engine & Async Engine =========================
class LLMEngine {
  +vllm_config: VllmConfig
  +model_executor: ExecutorBase
  +scheduler: list[Scheduler]
  +output_processor: SequenceGroupOutputProcessor
  +tokenizer: Optional[TokenizerGroup]
  +seq_id_to_seq_group: Dict[str, SequenceGroupBase]
  +add_request(...)
  +step()
  +abort_request(...)
  +add_lora(...)
  +remove_lora(...)
  +list_loras()
  +pin_lora(...)
  +start_profile()
  +stop_profile()
  +sleep()
  +wake_up()
  +is_sleeping()
  +check_health()
  +reset_mm_cache()
  +reset_prefix_cache()
}
class AsyncLLMEngine {
  +engine: _AsyncLLMEngine
  +request_tracker: RequestTracker
  +background_loop: Optional[asyncio.Future]
  +add_request(...)
  +generate(...)
  +encode(...)
  +abort(...)
  +start_background_loop()
  +shutdown_background_loop()
}
EngineClient <|-- AsyncLLMEngine

class _AsyncLLMEngine {
  +step_async(...)
  +add_request_async(...)
  +check_health_async()
  +collective_rpc_async(...)
}
LLMEngine <|-- _AsyncLLMEngine

%% ========================= Scheduler =========================
class Scheduler {
  +waiting: Deque[SequenceGroup]
  +running: Deque[SequenceGroup]
  +swapped: Deque[SequenceGroup]
  +block_manager: BlockSpaceManager
  +add_seq_group(...)
  +schedule()
  +fork_seq(...)
  +free_seq(...)
  +remove_seq_from_computed_blocks_tracker(...)
  +free_finished_seq_groups()
  +reset_prefix_cache()
  +get_prefix_cache_hit_rate()
}
class SchedulerOutputs {
  +scheduled_seq_groups: Sequence[ScheduledSequenceGroup]
  +blocks_to_swap_in: List[Tuple[int,int]]
  +blocks_to_swap_out: List[Tuple[int,int]]
  +blocks_to_copy: List[Tuple[int,int]]
  +ignored_seq_groups: List[SequenceGroup]
  +num_lookahead_slots: int
  +running_queue_size: int
  +preempted: int
  +num_prefill_groups
  +num_batched_tokens
}
class ScheduledSequenceGroup {
  +seq_group: SequenceGroup
  +token_chunk_size: int
}

class SchedulingBudget {
  +can_schedule(...)
  +remaining_token_budget()
  +add_num_batched_tokens(...)
  +subtract_num_batched_tokens(...)
  +add_num_seqs(...)
  +subtract_num_seqs(...)
}

class PreemptionMode { SWAP RECOMPUTE }
class SchedulerRunningOutputs
class SchedulerSwappedInOutputs
class SchedulerPrefillOutputs
class PartialPrefillMetadata

%% ========================= Output Processor =========================
class SequenceGroupOutputProcessor {
  <<abstract>>
  +create_output_processor(...)
  +process_outputs(sequence_group, outputs, is_async)
  +process_prompt_logprob(seq_group, outputs)
}
class SingleStepOutputProcessor {
  +__init__(scheduler_config, detokenizer, scheduler, seq_counter, stop_checker)
  +scheduler_config: SchedulerConfig
  +detokenizer: Detokenizer
  +scheduler: List[Scheduler]
  +seq_counter: Counter
  +stop_checker: StopChecker
  +process_outputs(sequence_group, outputs, is_async)
  +process_prompt_logprob(seq_group, outputs)
  +_process_sequence_group_outputs(seq_group, outputs, is_async)
}
SequenceGroupOutputProcessor <|-- SingleStepOutputProcessor

%% ========================= Sequences =========================
class SequenceGroup {
  +first_seq
  +sampling_params
  +prompt_logprobs
  +lora_request
  +get_seqs(status)
  +is_encoder_decoder()
  +get_encoder_seq()
}
class SequenceGroupOutput { +samples }
class CompletionSequenceGroupOutput { +prompt_logprobs +samples }
SequenceGroupOutput <|-- CompletionSequenceGroupOutput

class Sequence {
  +seq_id
  +append_token_id(token_id, logprobs, output_embed)
  +is_finished()
  +get_token_ids()
  +extra_hash()
  +data
}

%% ========================= Stop/Detok/Counter =========================
class StopChecker { +maybe_stop_sequence(seq, new_char_count, sampling_params, lora_req) }
class Detokenizer {
  +decode_sequence_inplace(seq, sampling_params)
  +decode_prompt_logprobs_inplace(seq_group, prompt_logprobs, position_offset)
}
class Counter

%% ========================= Block Manager & Allocators =========================
class BlockSpaceManager {
  <<abstract>>
  +can_allocate(...)
  +allocate(...)
  +free(...)
  +fork(...)
  +swap_in(...)
  +swap_out(...)
  +reset_prefix_cache(...)
  +get_prefix_cache_hit_rate(...)
}
class SelfAttnBlockSpaceManager {
  +__init__(block_size, num_gpu_blocks, num_cpu_blocks, ...)
  +block_allocator: CpuGpuBlockAllocator
  +block_tables: Dict[SeqId, BlockTable]
  +cross_block_tables: Dict[EncoderSeqId, BlockTable]
  +_computed_blocks_tracker: ComputedBlocksTracker
  +_last_access_blocks_tracker: LastAccessBlocksTracker
  +can_allocate(seq_group, num_lookahead_slots)
  +allocate(seq_group)
  +free(seq)
  +remove_seq_from_computed_blocks_tracker(seq)
  +free_cross(seq_group)
  +can_append_slots(seq_group, num_lookahead_slots)
  +append_slots(seq, num_lookahead_slots)
  +get_block_table(seq)
  +get_cross_block_table(seq_group)
  +access_all_blocks_in_seq(seq, now)
  +mark_blocks_as_computed(seq_group, token_chunk_size)
  +get_common_computed_block_ids(seqs)
  +fork(parent_seq, child_seq)
  +can_swap_in(seq_group, num_lookahead_slots)
  +swap_in(seq_group)
  +can_swap_out(seq_group)
  +swap_out(seq_group)
  +get_num_free_gpu_blocks()
  +get_num_free_cpu_blocks()
  +get_prefix_cache_hit_rate(device)
  +reset_prefix_cache(device)
  +_can_swap(seq_group, device, status, num_lookahead_slots)
  +get_num_cached_tokens(seq)
}
BlockSpaceManager <|-- SelfAttnBlockSpaceManager

class BlockAllocator {
  <<interface>>
  +allocate_mutable_block(...)
  +allocate_immutable_blocks(...)
  +allocate_immutable_block(...)
  +free(block)
  +fork(last_block)
  +get_num_free_blocks()
  +get_num_total_blocks()
  +get_physical_block_id(absolute_id)
  +swap_in(blocks)
  +swap_out(blocks)
  +get_num_full_blocks_touched(blocks)
  +clear_copy_on_writes()
  +mark_blocks_as_accessed(block_ids, now)
  +mark_blocks_as_computed(block_ids)
  +get_common_computed_block_ids(computed_seq_block_ids)
  +all_block_ids
  +get_prefix_cache_hit_rate()
  +reset_prefix_cache()
  +find_cached_blocks_prefix(block_hashes)
}
class DeviceAwareBlockAllocator {
  +allocate_mutable_block(...)
  +allocate_immutable_blocks(...)
  +allocate_immutable_block(...)
  +free(block)
  +fork(last_block)
  +get_num_free_blocks(device)
  +get_num_total_blocks(device)
  +get_physical_block_id(device, absolute_id)
  +swap(blocks, src_device, dst_device)
  +get_num_full_blocks_touched(blocks, device)
  +clear_copy_on_writes()
  +mark_blocks_as_accessed(block_ids, now)
  +mark_blocks_as_computed(block_ids)
  +get_common_computed_block_ids(computed_seq_block_ids)
  +all_block_ids
  +get_prefix_cache_hit_rate(device)
  +reset_prefix_cache(device)
  +get_and_reset_swaps()
  +find_cached_blocks_prefix(block_hashes, device)
}
class CpuGpuBlockAllocator {
  +create(...)
  +__init__(cpu_block_allocator, gpu_block_allocator)
  +allocate_or_get_null_block()
  +allocate_mutable_block(...)
  +allocate_immutable_blocks(...)
  +allocate_immutable_block(...)
  +free(block)
  +fork(last_block)
  +get_num_free_blocks(device)
  +get_num_total_blocks(device)
  +get_physical_block_id(device, absolute_id)
  +swap(blocks, src_device, dst_device)
  +get_num_full_blocks_touched(blocks, device)
  +clear_copy_on_writes()
  +mark_blocks_as_accessed(block_ids, now)
  +mark_blocks_as_computed(block_ids)
  +get_common_computed_block_ids(computed_seq_block_ids)
  +all_block_ids
  +get_prefix_cache_hit_rate(device)
  +reset_prefix_cache(device)
  +get_and_reset_swaps()
  +find_cached_blocks_prefix(block_hashes, device)
}
DeviceAwareBlockAllocator <|-- CpuGpuBlockAllocator

class BlockTable {
  +block_size: int
  +block_allocator: CpuGpuBlockAllocator
  +blocks: List[Block]
  +allocate(...)
  +fork(...)
  +update(...)
  +free(...)
  +get_num_blocks_touched_by_append_slots(...)
  +get_unseen_token_ids(...)
  +append_token_ids(...)
  +physical_block_ids: List[int]
}

class Block {
  +block_id: int
  +token_ids: List[int]
  +num_tokens_total: int
  +num_empty_slots: int
  +is_full: bool
  +prev_block: Optional[Block]
  +extra_hash: Optional[int]
  +computed: bool
  +last_accessed: float
  +content_hash: Optional[int]
  +append_token_ids(...)
}

class NullBlock {
  +__init__(proxy)
  +_proxy: Block
  +append_token_ids(token_ids)
  +block_id
  +token_ids
  +num_tokens_total
  +num_empty_slots
  +is_full
  +prev_block
  +extra_hash
  +computed
  +last_accessed
  +content_hash
}
Block <|-- NullBlock

class NaiveBlockAllocator
BlockAllocator <|-- NaiveBlockAllocator
class PrefixCachingBlockAllocator
BlockAllocator <|-- PrefixCachingBlockAllocator

%% ========================= Model Loader =========================
class BaseModelLoader {
  +load_config: LoadConfig
  +download_model(...)
  +load_weights(...)
  +load_model(...)
}
class DefaultModelLoader {
  +Source
  +counter_before_loading_weights: float
  +counter_after_loading_weights: float
  +_prepare_weights(...)
  +_get_weights_iterator(...)
  +get_all_weights(...)
  +download_model(...)
  +load_weights(...)
}
BaseModelLoader <|-- DefaultModelLoader

class GGUFModelLoader {
  +_prepare_weights(...)
  +_get_gguf_weights_map(...)
  +_get_weights_iterator(...)
  +download_model(...)
  +load_weights(...)
  +load_model(...)
}
BaseModelLoader <|-- GGUFModelLoader

%% ========================= Layers / Modules =========================
class QuantizeMethodBase
class CustomOp

class FusedMoEMethodBase {
  <<abstract>>
  +moe: FusedMoEConfig
  +create_weights(...)
  +uses_weight_scale_2_pattern()
  +maybe_make_prepare_finalize(...)
  +init_prepare_finalize(...)
  +select_gemm_impl(...)
  +maybe_swap_experts_impl(...)
  +apply(...)
}
QuantizeMethodBase <|-- FusedMoEMethodBase

class UnquantizedFusedMoEMethod {
  +select_gemm_impl(...)
  +create_weights(...)
  +_maybe_pad_weight(...)
  +process_weights_after_loading(...)
  +apply(...)
  +forward_cuda(...)
  +forward_cpu(...)
  +forward_xpu(...)
  +forward_tpu(...)
}
FusedMoEMethodBase <|-- UnquantizedFusedMoEMethod
CustomOp <|-- UnquantizedFusedMoEMethod

class FusedMoE {
  +moe_parallel_config: FusedMoEParallelConfig
  +global_num_experts: int
  +local_num_experts: int
  +quant_method: FusedMoEMethodBase
  +forward(...)
  +forward_impl_chunked(...)
  +forward_impl(...)
  +weight_loader(...)
  +get_expert_weights(...)
  +set_eplb_state(...)
  +must_reduce_shared_expert_outputs(...)
  +maybe_all_reduce_tensor_model_parallel(...)
}
torch.nn.Module <|-- FusedMoE

class LinearMethodBase { <<abstract>> +create_weights(...) +apply(...) }
QuantizeMethodBase <|-- LinearMethodBase

class UnquantizedLinearMethod {
  +create_weights(...)
  +process_weights_after_loading(...)
  +apply(...)
}
LinearMethodBase <|-- UnquantizedLinearMethod

class LinearBase {
  +input_size: int
  +output_size: int
  +skip_bias_add: bool
  +params_dtype: Optional[torch.dtype]
  +quant_config: Optional[QuantizationConfig]
  +prefix: str
  +return_bias: bool
  +forward(...)
}
torch.nn.Module <|-- LinearBase

class ReplicatedLinear { +output_partition_sizes: list[int] +quant_method: LinearMethodBase +weight_loader(...) +forward(...) }
LinearBase <|-- ReplicatedLinear
class MergedReplicatedLinear { +output_sizes: list[int] +weight_loader(...) +forward(...) }
ReplicatedLinear <|-- MergedReplicatedLinear

class ColumnParallelLinear { +weight_loader(...) +weight_loader_v2(...) +forward(...) }
LinearBase <|-- ColumnParallelLinear
class MergedColumnParallelLinear { +output_sizes: list[int] +weight_loader(...) +weight_loader_v2(...) +forward(...) }
ColumnParallelLinear <|-- MergedColumnParallelLinear

class QKVParallelLinear { +_get_shard_offset_mapping(...) +_get_shard_size_mapping(...) +weight_loader(...) +weight_loader_v2(...) +forward(...) }
ColumnParallelLinear <|-- QKVParallelLinear

class RowParallelLinear { +weight_loader(...) +weight_loader_v2(...) +forward(...) }
LinearBase <|-- RowParallelLinear

class QKVCrossParallelLinear { +sync_weight_attrs(...) +select_proj_params(...) +forward(...) +weight_loader(...) }
LinearBase <|-- QKVCrossParallelLinear

%% ========================= Worker / Cache Engine =========================
class CacheEngine {
  +__init__(cache_config, model_config, parallel_config, device_config)
  +_allocate_kv_cache(num_blocks, device)
  +swap_in(src_to_dst)
  +swap_out(src_to_dst)
  +copy(src_to_dsts)
  +get_cache_block_size(cache_config, model_config, parallel_config)
}

class WorkerBase {
  +vllm_config: VllmConfig
  +model_config
  +cache_config
  +lora_config
  +load_config
  +parallel_config
  +scheduler_config
  +device_config
  +speculative_config
  +observability_config
  +kv_transfer_config
  +compilation_config
  +current_platform
  +__init__(vllm_config)
  +init_device()
  +initialize_cache(...)
  +get_model()
  +load_model()
  +execute_model(...)
  +start_worker_execution_loop()
  +determine_num_available_blocks()
  +get_cache_block_size_bytes()
  +add_lora(...)
  +remove_lora(...)
  +pin_lora(...)
  +list_loras()
  +vocab_size
}
class DelegateWorkerBase {
  +worker: WorkerBase
  +__init__(*args, **kwargs)
  +init_device()
  +determine_num_available_blocks()
  +initialize_cache(...)
  +load_model()
  +get_model()
  +execute_model(...)
  +get_cache_block_size_bytes()
  +add_lora(...)
  +remove_lora(...)
  +pin_lora(...)
  +list_loras()
  +__getattr__(attr)
}
WorkerBase <|-- DelegateWorkerBase

class LoRANotSupportedWorkerBase { +add_lora(...) +remove_lora(...) +pin_lora(...) +list_loras() }
WorkerBase <|-- LoRANotSupportedWorkerBase

class LocalOrDistributedWorkerBase {
  +is_driver_worker: bool
  +model_runner: ModelRunnerBase
  +observability_config: Optional[ObservabilityConfig]
  +do_metadata_broadcast
  +kv_cache
  +prepare_worker_input(...)
  +execute_worker(...)
  +_get_worker_input_from_broadcast()
  +_get_driver_input_and_broadcast(...)
  +prepare_input(...)
  +get_model()
  +execute_model(...)
  +_execute_model_spmd(...)
}
WorkerBase <|-- LocalOrDistributedWorkerBase

class Worker {
  +__init__(vllm_config, local_rank, rank, distributed_init_method, ...)
  +local_rank: int
  +rank: int
  +distributed_init_method: str
  +is_driver_worker: bool
  +model_runner: GPUModelRunnerBase
  +cache_engine: List[CacheEngine]
  +gpu_cache: Optional[List[List[torch.Tensor]]]
  +_seq_group_metadata_cache: Dict[str, SequenceGroupMetadata]
  +_sleep_saved_buffers: Dict[str, torch.Tensor]
  +profiler
  +start_profile()
  +stop_profile()
  +sleep(level)
  +wake_up(tags)
  +init_device()
  +load_model()
  +save_sharded_state(path, pattern, max_size)
  +save_tensorized_model(tensorizer_config)
  +determine_num_available_blocks()
  +_assert_memory_footprint_increased_during_profiling()
  +initialize_cache(num_gpu_blocks, num_cpu_blocks)
  +_init_cache_engine()
  +_warm_up_model()
  +do_metadata_broadcast
  +kv_cache
  +prepare_worker_input(execute_model_req)
  +execute_worker(worker_input)
  +_get_cached_seq_group_metadata(seq_group_metadata_list, finished_request_ids)
  +_execute_model_spmd(execute_model_req, intermediate_tensors)
  +add_lora(lora_request)
  +remove_lora(lora_id)
  +pin_lora(lora_id)
  +list_loras()
  +max_model_len
  +vocab_size
  +get_cache_block_size_bytes()
}
LocalOrDistributedWorkerBase <|-- Worker

%% ========================= Executors =========================
class ExecutorBase
class Executor {
  +get_class(vllm_config)
  +initialize_from_config(kv_cache_configs)
  +register_failure_callback(callback)
  +determine_available_memory()
  +get_kv_cache_specs()
  +execute_model(scheduler_output)
  +max_concurrent_batches
  +profile(is_start)
}
ExecutorBase <|-- Executor

class UniProcExecutor
UniProcExecutorV0 <|-- UniProcExecutor
Executor <|-- UniProcExecutor

class ExecutorWithExternalLauncher
ExecutorWithExternalLauncherV0 <|-- ExecutorWithExternalLauncher
Executor <|-- ExecutorWithExternalLauncher

class KVOutputAggregator
class FutureWrapper { +refs +aggregator: Optional[KVOutputAggregator] +result(timeout) }
class RayDistributedExecutorV0
class RayDistributedExecutor {
  +supports_pp: bool
  +_init_executor()
  +max_concurrent_batches
  +execute_model(...)
  +reinitialize_distributed(...)
  +kv_output_aggregator: KVOutputAggregator
}
RayDistributedExecutorV0 <|-- RayDistributedExecutor
Executor <|-- RayDistributedExecutor
Future <|-- FutureWrapper

class MultiprocExecutor {
  +supports_pp: bool
  +_init_executor()
  +start_worker_monitor()
  +register_failure_callback(...)
  +execute_model(...)
  +collective_rpc(...)
  +shutdown()
  +check_health()
  +max_concurrent_batches
  +_get_output_rank()
  +workers: list[WorkerProcHandle]
  +rpc_broadcast_mq: MessageQueue
  +io_thread_pool: Optional[ThreadPoolExecutor]
  +failure_callback: Optional[FailureCallback]
  +kv_output_aggregator: KVOutputAggregator
}
Executor <|-- MultiprocExecutor

class WorkerProc {
  +READY_STR
  +rpc_broadcast_mq: MessageQueue
  +worker_response_mq: MessageQueue
  +worker: WorkerWrapperBase
  +make_worker_process(...)
  +wait_for_ready(...)
  +shutdown()
  +worker_main(...)
  +worker_busy_loop()
  +ResponseStatus(Enum)
}
class WorkerProcHandle {
  +proc: BaseProcess
  +rank: int
  +worker_response_mq: MessageQueue
  +death_writer: Optional[Connection]
  +from_unready_handle(...)
}
class UnreadyWorkerProcHandle {
  +proc: BaseProcess
  +rank: int
  +ready_pipe: Connection
  +death_writer: Optional[Connection]
}
class WorkerWrapperBase {
  +rpc_rank
  +worker: Optional[WorkerBase]
  +vllm_config: Optional[VllmConfig]
  +adjust_rank(...)
  +update_environment_variables(...)
  +init_worker(...)
  +initialize_from_config(...)
  +init_device()
  +execute_method(...)
  +__getattr__(...)
}

%% ========================= Serving =========================
class OpenAIServing {
  +engine_client: EngineClient
  +model_config: ModelConfig
  +models: OpenAIServingModels
  +request_logger: Optional[RequestLogger]
  +_preprocess(...)
  +_build_response(...)
  +handle(...)
  +_pipeline(...)
  +_validate_request(...)
  +_create_pooling_params(...)
  +_prepare_generators(...)
  +_collect_batch(...)
  +create_error_response(...)
  +create_streaming_error_response(...)
  +_check_model(...)
  +_maybe_get_adapters(...)
  +_get_message_types(...)
  +_normalize_prompt_text_to_input(...)
  +_normalize_prompt_tokens_to_input(...)
  +_validate_input(...)
  +_tokenize_prompt_input_async(...)
  +_tokenize_prompt_inputs_async(...)
  +_tokenize_prompt_input_or_inputs_async(...)
  +_preprocess_completion(...)
  +_preprocess_chat(...)
  +_generate_with_builtin_tools(...)
  +_load_prompt_embeds(...)
  +_log_inputs(...)
  +_get_trace_headers(...)
  +_is_model_supported(...)
  +_get_model_name(...)
}
class EmbeddingMixin { +_preprocess(...) +_build_response(...) }
OpenAIServing <|-- EmbeddingMixin
class OpenAIServingEmbedding {
  +request_id_prefix: str
  +create_embedding(...)
  +_validate_request(...)
  +_create_pooling_params(...)
}
EmbeddingMixin <|-- OpenAIServingEmbedding

%% ========================= Relationships (High-level) =========================
LLMEngine --> Scheduler : manages
LLMEngine --> SequenceGroupOutputProcessor : uses
LLMEngine --> ExecutorBase : uses
LLMEngine --> TokenizerGroup : uses
LLMEngine --> SequenceGroupBase : manages
LLMEngine --> StatLoggerBase : logs
LLMEngine --> StopChecker : uses
LLMEngine --> Detokenizer : uses
LLMEngine --> LoRARequest : manages
LLMEngine --> RequestOutput : returns
LLMEngine --> PoolingRequestOutput : returns

Scheduler --> BlockSpaceManager : manages
Scheduler --> SequenceGroup : schedules
Scheduler --> ScheduledSequenceGroup : uses
Scheduler --> SchedulerOutputs : returns
Scheduler --> SchedulingBudget : budget
Scheduler --> PreemptionMode : mode
Scheduler --> SequenceGroupOutputProcessor : output_proc_callback

BlockSpaceManager --> CpuGpuBlockAllocator : uses
CpuGpuBlockAllocator --> BlockAllocator : uses
BlockAllocator --> Block : allocates
BlockTable --> Block : manages
BlockTable --> CpuGpuBlockAllocator : uses
BlockTable --> BlockTable : forks

SelfAttnBlockSpaceManager --> CpuGpuBlockAllocator : block_allocator
SelfAttnBlockSpaceManager --> BlockTable : block_tables
CpuGpuBlockAllocator --> NullBlock : allocate_or_get_null_block
CpuGpuBlockAllocator --> Block : allocate_mutable_block, allocate_immutable_block

SingleStepOutputProcessor --> Detokenizer : detokenizer
SingleStepOutputProcessor --> Scheduler : scheduler
SingleStepOutputProcessor --> Counter : seq_counter
SingleStepOutputProcessor --> StopChecker : stop_checker
SingleStepOutputProcessor --> SequenceGroup : process_outputs
SingleStepOutputProcessor --> SequenceGroupOutput : process_outputs
SingleStepOutputProcessor --> CompletionSequenceGroupOutput : process_prompt_logprob

Worker --> GPUModelRunnerBase : model_runner
Worker --> CacheEngine : cache_engine
Worker --> SequenceGroupMetadata : _seq_group_metadata_cache
Worker --> SamplerOutput : returns
Worker --> LoRARequest : manages
Worker --> TensorizerConfig : save_tensorized_model
Worker --> VllmConfig : vllm_config

MultiprocExecutor --> WorkerProc : manages
MultiprocExecutor --> WorkerProcHandle : manages
MultiprocExecutor --> UnreadyWorkerProcHandle : manages

RayDistributedExecutor --> KVOutputAggregator : uses
RayDistributedExecutor --> FutureWrapper : returns

OpenAIServingEmbedding --> EmbeddingServeContext : uses
OpenAIServingEmbedding --> EmbeddingRequest : uses
OpenAIServingEmbedding --> EmbeddingResponse : returns
