classDiagram
%% ========================= Enums / Interfaces / ABC =========================
class ABC

class _Backend {
  <<enum>>
  FLASH_ATTN
  FLASH_ATTN_VLLM_V1
  TRITON_ATTN_VLLM_V1
  XFORMERS
  ROCM_FLASH
  ROCM_AITER_MLA
  ROCM_AITER_MLA_VLLM_V1
  ROCM_AITER_FA
  TORCH_SDPA
  FLASHINFER
  FLASHINFER_VLLM_V1
  TRITON_MLA
  TRITON_MLA_VLLM_V1
  FLASHMLA_VLLM_V1
  FLASHMLA
  CUTLASS_MLA
  PALLAS
  PALLAS_VLLM_V1
  IPEX
  DUAL_CHUNK_FLASH_ATTN
  DIFFERENTIAL_FLASH_ATTN
  NO_ATTENTION
  FLEX_ATTENTION
  TREE_ATTN
  XFORMERS_VLLM_V1
}

class PlatformEnum {
  <<enum>>
  CUDA
  ROCM
  TPU
  XPU
  CPU
  NEURON
  OOT
  UNSPECIFIED
}

class CpuArchEnum {
  <<enum>>
  X86
  ARM
  POWERPC
  OTHER
  UNKNOWN
}

class DeviceCapability {
  +major: int
  +minor: int
  +as_version_str(): str
  +to_int(): int
}

%% ========================= Output Processor =========================
class SequenceGroupOutputProcessor {
  <<interface>>
  +create_output_processor(...)
  +process_outputs(sequence_group, outputs, is_async)
  +process_prompt_logprob(seq_group, outputs)
}

class SingleStepOutputProcessor {
  +__init__(scheduler_config, detokenizer, scheduler, seq_counter, stop_checker)
  +scheduler_config: SchedulerConfig
  +process_outputs(sequence_group, outputs, is_async)
  +process_prompt_logprob(seq_group, outputs)
  +_process_sequence_group_outputs(seq_group, outputs, is_async)
  -detokenizer: Detokenizer
  -scheduler: List[Scheduler]
  -seq_counter: Counter
  -stop_checker: StopChecker
}
SequenceGroupOutputProcessor <|.. SingleStepOutputProcessor
SingleStepOutputProcessor o-- Detokenizer
SingleStepOutputProcessor o-- Scheduler
SingleStepOutputProcessor o-- Counter
SingleStepOutputProcessor o-- StopChecker
SingleStepOutputProcessor --> SequenceGroup : process_outputs
SingleStepOutputProcessor --> SequenceGroupOutput : process_outputs
SingleStepOutputProcessor --> CompletionSequenceGroupOutput : process_prompt_logprob

%% ========================= Engine & Async =========================
class LLMEngine {
  +vllm_config: VllmConfig
  +model_executor: ExecutorBase
  +scheduler: list[Scheduler]
  +output_processor: SequenceGroupOutputProcessor
  +tokenizer: Optional[TokenizerGroup]
  +seq_id_to_seq_group: Dict[str, SequenceGroupBase]
  +add_request(...)
  +abort_request(...)
  +step()
  +collective_rpc(...)
  +add_lora(...); +remove_lora(...); +list_loras(); +pin_lora(...)
  +start_profile(); +stop_profile()
  +sleep(); +wake_up(); +is_sleeping()
  +check_health()
  +reset_mm_cache(); +reset_prefix_cache()
  +is_tracing_enabled(); +do_tracing(...); +create_trace_span(seq_group)
}
LLMEngine o-- ExecutorBase
LLMEngine o-- Scheduler
LLMEngine o-- SequenceGroupOutputProcessor
LLMEngine o-- SequenceGroupBase
LLMEngine o-- TokenizerGroup

class AsyncLLMEngine {
  +engine: _AsyncLLMEngine
  +request_tracker: RequestTracker
  +background_loop: Optional[asyncio.Future]
  +add_request(...); +generate(...); +encode(...); +abort(...)
  +start_background_loop(); +shutdown_background_loop()
}
EngineClient <|-- AsyncLLMEngine
AsyncLLMEngine o-- LLMEngine
AsyncLLMEngine o-- RequestTracker
AsyncLLMEngine o-- AsyncStream

class _AsyncLLMEngine {
  +step_async(...)
  +add_request_async(...)
  +check_health_async()
  +collective_rpc_async(...)
}
LLMEngine <|-- _AsyncLLMEngine

class RequestTracker {
  +add_request(...); +abort_request(...)
  +process_request_output(...); +process_exception(...)
  +get_new_and_aborted_requests()
  -_request_streams: Dict[str, AsyncStream]
  -_aborted_requests: asyncio.Queue[str]
  -_new_requests: asyncio.Queue[Tuple[AsyncStream, dict]]
}
RequestTracker o-- AsyncStream

class AsyncStream { +put(item) +finish(exception) +generator() -_queue: asyncio.Queue }

%% ========================= Scheduler & Sequences =========================
class Scheduler {
  +schedule()
  +add_seq_group(seq_group)
  +free_seq(seq)
  +fork_seq(parent_seq, child_seq)
  +remove_seq_from_computed_blocks_tracker(seq)
  +free_finished_seq_groups()
  +get_num_unfinished_seq_groups()
  +get_and_reset_finished_requests_ids()
  +abort_seq_group(request_id, seq_id_to_seq_group)
  +get_prefix_cache_hit_rate(device)
  +reset_prefix_cache(device)
  +has_unfinished_seqs()
  +block_manager: BlockSpaceManager
  +waiting: Deque[SequenceGroup]
  +running: Deque[SequenceGroup]
  +swapped: Deque[SequenceGroup]
}
class SchedulingBudget {
  +can_schedule(...)
  +remaining_token_budget()
  +add_num_batched_tokens(...); +subtract_num_batched_tokens(...)
  +add_num_seqs(...); +subtract_num_seqs(...)
}
class PreemptionMode { SWAP RECOMPUTE }
class PartialPrefillMetadata
class SchedulerOutputs {
  +scheduled_seq_groups: Sequence[ScheduledSequenceGroup]
  +blocks_to_swap_in: List[Tuple[int,int]]
  +blocks_to_swap_out: List[Tuple[int,int]]
  +blocks_to_copy: List[Tuple[int,int]]
  +ignored_seq_groups: List[SequenceGroup]
  +num_lookahead_slots: int
  +running_queue_size: int
  +preempted: int
  +num_prefill_groups
  +num_batched_tokens
}
class ScheduledSequenceGroup { +seq_group: SequenceGroup +token_chunk_size: int }

Scheduler o-- BlockSpaceManager
Scheduler o-- SequenceGroup
Scheduler o-- SequenceGroupMetadata
Scheduler o-- SchedulingBudget
Scheduler o-- SchedulerOutputs
Scheduler o-- PartialPrefillMetadata
Scheduler --> ScheduledSequenceGroup
Scheduler --> PreemptionMode
Scheduler --> SequenceGroupOutputProcessor : output_proc_callback

class SequenceGroup {
  +first_seq
  +sampling_params
  +prompt_logprobs
  +lora_request
  +get_seqs(status)
  +is_encoder_decoder()
  +get_encoder_seq()
}
class SequenceGroupOutput { +samples }
class CompletionSequenceGroupOutput { +prompt_logprobs +samples }
SequenceGroupOutput <|-- CompletionSequenceGroupOutput

class Sequence {
  +seq_id
  +append_token_id(token_id, logprobs, output_embed)
  +is_finished()
  +get_token_ids()
  +extra_hash()
  +data
}

%% ========================= Stop / Detokenizer / Counter =========================
class StopChecker { +maybe_stop_sequence(seq, new_char_count, sampling_params, lora_req) }
class Detokenizer {
  +decode_sequence_inplace(seq, sampling_params)
  +decode_prompt_logprobs_inplace(seq_group, prompt_logprobs, position_offset)
}
class Counter

%% ========================= Block Manager & Allocators =========================
class BlockSpaceManager {
  <<interface>>
  +can_allocate(seq_group, num_lookahead_slots)
  +allocate(seq_group)
  +free(seq)
  +fork(parent_seq, child_seq)
  +swap_in(seq_group)
  +swap_out(seq_group)
  +get_prefix_cache_hit_rate(device)
  +reset_prefix_cache(device)
}
class SelfAttnBlockSpaceManager {
  +__init__(block_size, num_gpu_blocks, num_cpu_blocks, ...)
  +block_allocator: CpuGpuBlockAllocator
  +block_tables: Dict[SeqId, BlockTable]
  +cross_block_tables: Dict[EncoderSeqId, BlockTable]
  +_computed_blocks_tracker: ComputedBlocksTracker
  +_last_access_blocks_tracker: LastAccessBlocksTracker
  +can_allocate(seq_group, num_lookahead_slots)
  +allocate(seq_group); +free(seq); +fork(parent_seq, child_seq)
  +remove_seq_from_computed_blocks_tracker(seq)
  +free_cross(seq_group)
  +can_append_slots(seq_group, num_lookahead_slots); +append_slots(seq, num_lookahead_slots)
  +get_block_table(seq); +get_cross_block_table(seq_group)
  +access_all_blocks_in_seq(seq, now)
  +mark_blocks_as_computed(seq_group, token_chunk_size)
  +get_common_computed_block_ids(seqs)
  +can_swap_in(seq_group, num_lookahead_slots); +swap_in(seq_group)
  +can_swap_out(seq_group); +swap_out(seq_group)
  +get_num_free_gpu_blocks(); +get_num_free_cpu_blocks()
  +get_prefix_cache_hit_rate(device); +reset_prefix_cache(device)
  +_can_swap(seq_group, device, status, num_lookahead_slots)
  +get_num_cached_tokens(seq)
}
BlockSpaceManager <|.. SelfAttnBlockSpaceManager
SelfAttnBlockSpaceManager o-- CpuGpuBlockAllocator
SelfAttnBlockSpaceManager o-- BlockTable

class BlockAllocator {
  <<interface>>
  +allocate_mutable_block(...)
  +allocate_immutable_blocks(...)
  +allocate_immutable_block(...)
  +free(block)
  +fork(last_block)
  +swap_in(blocks); +swap_out(blocks)
  +get_num_free_blocks(...); +get_num_total_blocks(...)
  +get_physical_block_id(...)
  +reset_prefix_cache(...); +get_prefix_cache_hit_rate(...)
  +find_cached_blocks_prefix(block_hashes)
}
class DeviceAwareBlockAllocator {
  +allocate_mutable_block(...); +allocate_immutable_blocks(...); +allocate_immutable_block(...)
  +free(block); +fork(last_block)
  +get_num_free_blocks(device); +get_num_total_blocks(device)
  +get_physical_block_id(device, absolute_id)
  +swap(blocks, src_device, dst_device)
  +get_num_full_blocks_touched(blocks, device)
  +clear_copy_on_writes()
  +mark_blocks_as_accessed(block_ids, now)
  +mark_blocks_as_computed(block_ids)
  +get_common_computed_block_ids(computed_seq_block_ids)
  +all_block_ids
  +get_prefix_cache_hit_rate(device); +reset_prefix_cache(device)
  +get_and_reset_swaps()
  +find_cached_blocks_prefix(block_hashes, device)
}
class CpuGpuBlockAllocator {
  +create(...); +__init__(cpu_block_allocator, gpu_block_allocator)
  +allocate_or_get_null_block()
  +allocate_mutable_block(...); +allocate_immutable_blocks(...); +allocate_immutable_block(...)
  +free(block); +fork(last_block)
  +get_num_free_blocks(device); +get_num_total_blocks(device)
  +get_physical_block_id(device, absolute_id)
  +swap(blocks, src_device, dst_device)
  +get_num_full_blocks_touched(blocks, device)
  +clear_copy_on_writes()
  +mark_blocks_as_accessed(block_ids, now)
  +mark_blocks_as_computed(block_ids)
  +get_common_computed_block_ids(computed_seq_block_ids)
  +all_block_ids
  +get_prefix_cache_hit_rate(device); +reset_prefix_cache(device)
  +get_and_reset_swaps()
  +find_cached_blocks_prefix(block_hashes, device)
  -_allocators: Dict[Device, BlockAllocator]
  -_block_ids_to_allocator: Dict[int, BlockAllocator]
  -_null_block: Optional[Block]
}
DeviceAwareBlockAllocator <|-- CpuGpuBlockAllocator
CpuGpuBlockAllocator o-- BlockAllocator
CpuGpuBlockAllocator o-- NullBlock

class BlockTable {
  +block_size: int
  +block_allocator: CpuGpuBlockAllocator
  +blocks: List[Block]
  +allocate(...); +fork(...); +update(...); +free(...)
  +get_num_blocks_touched_by_append_slots(...)
  +get_unseen_token_ids(...)
  +append_token_ids(...)
  +physical_block_ids: List[int]
}
class Block {
  +block_id: int
  +token_ids: List[int]
  +num_tokens_total: int
  +num_empty_slots: int
  +is_full: bool
  +prev_block: Optional[Block]
  +extra_hash: Optional[int]
  +computed: bool
  +last_accessed: float
  +content_hash: Optional[int]
  +append_token_ids(...)
}
class NullBlock {
  +__init__(proxy)
  +append_token_ids(token_ids)
  +block_id
  +token_ids
  +num_tokens_total
  +num_empty_slots
  +is_full
  +prev_block
  +extra_hash
  +computed
  +last_accessed
  +content_hash
  -_proxy: Block
}
Block <|-- NullBlock

class NaiveBlockAllocator
BlockAllocator <|.. NaiveBlockAllocator
class PrefixCachingBlockAllocator
BlockAllocator <|.. PrefixCachingBlockAllocator

%% ========================= Worker & Cache =========================
class CacheEngine {
  +__init__(cache_config, model_config, parallel_config, device_config)
  +_allocate_kv_cache(num_blocks, device)
  +swap_in(src_to_dst)
  +swap_out(src_to_dst)
  +copy(src_to_dsts)
  +get_cache_block_size(cache_config, model_config, parallel_config)
  -gpu_cache: List[torch.Tensor]
  -cpu_cache: List[torch.Tensor]
  -attn_backend
}
CacheEngine o-- torch.Tensor

class WorkerBase {
  +vllm_config: VllmConfig
  +model_config
  +cache_config
  +lora_config
  +load_config
  +parallel_config
  +scheduler_config
  +device_config
  +speculative_config
  +observability_config
  +kv_transfer_config
  +compilation_config
  +current_platform
  +__init__(vllm_config)
  +init_device()
  +initialize_cache(num_gpu_blocks, num_cpu_blocks)
  +get_model(); +load_model()
  +execute_model(execute_model_req)
  +start_worker_execution_loop()
  +determine_num_available_blocks()
  +get_cache_block_size_bytes()
  +add_lora(lora_request); +remove_lora(lora_id); +pin_lora(lora_id); +list_loras()
  +vocab_size
}
class DelegateWorkerBase {
  +worker: WorkerBase
  +__init__(*args, **kwargs)
  +init_device()
  +determine_num_available_blocks()
  +initialize_cache(...)
  +load_model(); +get_model()
  +execute_model(...)
  +get_cache_block_size_bytes()
  +add_lora(...); +remove_lora(...); +pin_lora(...); +list_loras()
  +__getattr__(attr)
}
WorkerBase <|-- DelegateWorkerBase

class LoRANotSupportedWorkerBase { +add_lora(...); +remove_lora(...); +pin_lora(...); +list_loras() }
WorkerBase <|-- LoRANotSupportedWorkerBase

class LocalOrDistributedWorkerBase {
  +is_driver_worker: bool
  +model_runner: ModelRunnerBase
  +observability_config: Optional[ObservabilityConfig]
  +do_metadata_broadcast
  +kv_cache
  +prepare_worker_input(execute_model_req)
  +execute_worker(worker_input)
  +_get_worker_input_from_broadcast()
  +_get_driver_input_and_broadcast(...)
  +prepare_input(execute_model_req)
  +get_model()
  +execute_model(execute_model_req)
  +_execute_model_spmd(execute_model_req, intermediate_tensors)
}
WorkerBase <|-- LocalOrDistributedWorkerBase

class Worker {
  +parallel_config: ParallelConfig
  +local_rank: int
  +rank: int
  +distributed_init_method: str
  +is_driver_worker: bool
  +model_runner: GPUModelRunnerBase
  +cache_engine: List[CacheEngine]
  +gpu_cache: Optional[List[List[torch.Tensor]]]
  +_seq_group_metadata_cache: Dict[str, SequenceGroupMetadata]
  +_sleep_saved_buffers: Dict[str, torch.Tensor]
  +profiler
  +baseline_snapshot
  +__init__(...)
  +start_profile(); +stop_profile()
  +sleep(level); +wake_up(tags)
  +init_device(); +load_model()
  +save_sharded_state(path, pattern, max_size)
  +save_tensorized_model(tensorizer_config)
  +determine_num_available_blocks(): Tuple[int,int]
  +_assert_memory_footprint_increased_during_profiling()
  +initialize_cache(num_gpu_blocks, num_cpu_blocks)
  +_init_cache_engine(); +_warm_up_model()
  +do_metadata_broadcast
  +kv_cache: Optional[List[List[torch.Tensor]]]
  +prepare_worker_input(execute_model_req): WorkerInput
  +execute_worker(worker_input)
  +_get_cached_seq_group_metadata(seq_group_metadata_list, finished_request_ids): List[SequenceGroupMetadata]
  +_execute_model_spmd(execute_model_req, intermediate_tensors): Optional[List[SamplerOutput]]
  +add_lora(lora_request); +remove_lora(lora_id); +pin_lora(lora_id); +list_loras()
  +max_model_len; +vocab_size; +get_cache_block_size_bytes()
}
LocalOrDistributedWorkerBase <|-- Worker
Worker o-- GPUModelRunnerBase
Worker o-- CacheEngine
Worker o-- SequenceGroupMetadata
Worker --> ParallelConfig : parallel_config
Worker --> torch.Tensor : gpu_cache
Worker --> torch.Tensor : _sleep_saved_buffers
Worker --> torch.profiler.profile : profiler
Worker --> MemorySnapshot : baseline_snapshot
Worker --> TensorizerConfig : save_tensorized_model()
Worker --> LoRARequest : add_lora()
Worker --> SamplerOutput : _execute_model_spmd()
Worker --> ExecuteModelRequest : prepare_worker_input()
Worker --> WorkerInput : prepare_worker_input()
Worker --> IntermediateTensors : _execute_model_spmd()
Worker --> VllmConfig : __init__()
Worker --> Attention : get_layers_from_vllm_config()
Worker --> CuMemAllocator : sleep(), wake_up(), load_model(), initialize_cache()
Worker --> bind_kv_cache : _init_cache_engine()
Worker --> memory_profiling : determine_num_available_blocks()
Worker --> current_platform : init_device(), _check_if_gpu_supports_dtype()
Worker --> CacheEngine.get_cache_block_size : get_cache_block_size_bytes()
Worker --> logger : logging
Worker --> gc : garbage collection
Worker --> os : environment variables
Worker --> torch : device, memory, profiler
Worker --> contextlib.nullcontext : load_model(), initialize_cache()
Worker --> set_random_seed : init_device(), _warm_up_model()
Worker --> init_worker_distributed_environment : init_device()
Worker --> raise_if_cache_size_invalid : initialize_cache()

class WorkerInput {
  +num_seq_groups: Optional[int]
  +blocks_to_swap_in: Optional[torch.Tensor]
  +blocks_to_swap_out: Optional[torch.Tensor]
  +blocks_to_copy: Optional[torch.Tensor]
  +virtual_engine: int
  +num_steps: int
  +from_broadcasted_tensor_dict(tensor_dict)
  +as_broadcastable_tensor_dict()
}

%% ========================= Executors =========================
class ExecutorBase
class Executor {
  +get_class(vllm_config)
  +initialize_from_config(kv_cache_configs)
  +register_failure_callback(callback)
  +determine_available_memory()
  +get_kv_cache_specs()
  +execute_model(scheduler_output)
  +max_concurrent_batches
  +profile(is_start)
}
ExecutorBase <|-- Executor

class UniProcExecutor
Executor <|-- UniProcExecutor
class UniProcExecutorV0
UniProcExecutorV0 <|-- UniProcExecutor

class ExecutorWithExternalLauncher
Executor <|-- ExecutorWithExternalLauncher
class ExecutorWithExternalLauncherV0
ExecutorWithExternalLauncherV0 <|-- ExecutorWithExternalLauncher

class KVOutputAggregator
class FutureWrapper { +refs +aggregator: Optional[KVOutputAggregator] +result(timeout) }

class RayDistributedExecutorV0
class RayDistributedExecutor {
  +supports_pp: bool
  +_init_executor()
  +max_concurrent_batches
  +execute_model(...)
  +reinitialize_distributed(...)
  +kv_output_aggregator: KVOutputAggregator
}
RayDistributedExecutorV0 <|-- RayDistributedExecutor
Executor <|-- RayDistributedExecutor
Future <|-- FutureWrapper

class MultiprocExecutor {
  +supports_pp: bool
  +_init_executor()
  +start_worker_monitor()
  +register_failure_callback(...)
  +execute_model(...)
  +collective_rpc(...)
  +shutdown(); +check_health()
  +max_concurrent_batches; +_get_output_rank()
  +workers: list[WorkerProcHandle]
  +rpc_broadcast_mq: MessageQueue
  +io_thread_pool: Optional[ThreadPoolExecutor]
  +failure_callback: Optional[FailureCallback]
  +kv_output_aggregator: KVOutputAggregator
}
Executor <|-- MultiprocExecutor

class WorkerProc {
  +READY_STR
  +rpc_broadcast_mq: MessageQueue
  +worker_response_mq: MessageQueue
  +worker: WorkerWrapperBase
  +make_worker_process(...); +wait_for_ready(...); +shutdown()
  +worker_main(...); +worker_busy_loop()
  +ResponseStatus(Enum)
}
class WorkerProcHandle { +proc: BaseProcess +rank: int +worker_response_mq: MessageQueue +death_writer: Optional[Connection] +from_unready_handle(...) }
class UnreadyWorkerProcHandle { +proc: BaseProcess +rank: int +ready_pipe: Connection +death_writer: Optional[Connection] }
class WorkerWrapperBase {
  +rpc_rank
  +worker: Optional[WorkerBase]
  +vllm_config: Optional[VllmConfig]
  +adjust_rank(...); +update_environment_variables(...)
  +init_worker(...); +initialize_from_config(...)
  +init_device(); +execute_method(method, *args, **kwargs)
  +__getattr__(...)
}
WorkerProc o-- WorkerWrapperBase
WorkerProc o-- MessageQueue

class WorkerProc.ResponseStatus { SUCCESS FAILURE }

%% ========================= Model Loader =========================
class BaseModelLoader {
  +load_config: LoadConfig
  +download_model(...)
  +load_weights(...)
  +load_model(...)
}
class DefaultModelLoader {
  +Source
  +counter_before_loading_weights: float
  +counter_after_loading_weights: float
  +_prepare_weights(...)
  +_get_weights_iterator(...)
  +get_all_weights(...)
  +download_model(...)
  +load_weights(...)
}
BaseModelLoader <|-- DefaultModelLoader

class GGUFModelLoader {
  +_prepare_weights(...)
  +_get_gguf_weights_map(...)
  +_get_weights_iterator(...)
  +download_model(...)
  +load_weights(...)
  +load_model(...)
}
BaseModelLoader <|-- GGUFModelLoader

%% ========================= Layers / Quant / Fused MoE =========================
class QuantizeMethodBase
class CustomOp

class FusedMoEMethodBase {
  <<abstract>>
  +moe: FusedMoEConfig
  +create_weights(...)
  +uses_weight_scale_2_pattern()
  +maybe_make_prepare_finalize(...)
  +init_prepare_finalize(...)
  +select_gemm_impl(...)
  +maybe_swap_experts_impl(...)
  +apply(...)
}
QuantizeMethodBase <|-- FusedMoEMethodBase

class UnquantizedFusedMoEMethod {
  +select_gemm_impl(...)
  +create_weights(...)
  +_maybe_pad_weight(...)
  +process_weights_after_loading(...)
  +apply(...)
  +forward_cuda(...); +forward_cpu(...); +forward_xpu(...); +forward_tpu(...)
}
FusedMoEMethodBase <|-- UnquantizedFusedMoEMethod
CustomOp <|-- UnquantizedFusedMoEMethod

class FusedMoE {
  +moe_parallel_config: FusedMoEParallelConfig
  +global_num_experts: int
  +local_num_experts: int
  +quant_method: FusedMoEMethodBase
  +forward(...); +forward_impl_chunked(...); +forward_impl(...)
  +weight_loader(...); +get_expert_weights(...); +set_eplb_state(...)
  +must_reduce_shared_expert_outputs(...); +maybe_all_reduce_tensor_model_parallel(...)
}
torch.nn.Module <|-- FusedMoE

class LinearMethodBase { <<abstract>> +create_weights(...) +apply(...) }
QuantizeMethodBase <|-- LinearMethodBase
class UnquantizedLinearMethod { +create_weights(...) +process_weights_after_loading(...) +apply(...) }
LinearMethodBase <|-- UnquantizedLinearMethod

class LinearBase {
  +input_size: int
  +output_size: int
  +skip_bias_add: bool
  +params_dtype: Optional[torch.dtype]
  +quant_config: Optional[QuantizationConfig]
  +prefix: str
  +return_bias: bool
  +forward(...)
}
torch.nn.Module <|-- LinearBase
class ReplicatedLinear { +output_partition_sizes: list[int] +quant_method: LinearMethodBase +weight_loader(...) +forward(...) }
LinearBase <|-- ReplicatedLinear
class MergedReplicatedLinear { +output_sizes: list[int] +weight_loader(...) +forward(...) }
ReplicatedLinear <|-- MergedReplicatedLinear
class ColumnParallelLinear { +weight_loader(...) +weight_loader_v2(...) +forward(...) }
LinearBase <|-- ColumnParallelLinear
class MergedColumnParallelLinear { +output_sizes: list[int] +weight_loader(...) +weight_loader_v2(...) +forward(...) }
ColumnParallelLinear <|-- MergedColumnParallelLinear
class QKVParallelLinear { +_get_shard_offset_mapping(...) +_get_shard_size_mapping(...) +weight_loader(...) +weight_loader_v2(...) +forward(...) }
ColumnParallelLinear <|-- QKVParallelLinear
class RowParallelLinear { +weight_loader(...) +weight_loader_v2(...) +forward(...) }
LinearBase <|-- RowParallelLinear
class QKVCrossParallelLinear { +sync_weight_attrs(...) +select_proj_params(...) +forward(...) +weight_loader(...) }
LinearBase <|-- QKVCrossParallelLinear

%% ========================= Model (Deepseek example) =========================
class DeepseekModel {
  +get_input_embeddings(input_ids)
  +forward(input_ids, positions, intermediate_tensors, inputs_embeds)
  +load_weights(weights)
  -embed_tokens: VocabParallelEmbedding
  -layers: List[DeepseekDecoderLayer]
  -norm: RMSNorm
}
DeepseekModel *-- DeepseekDecoderLayer
DeepseekModel o-- VocabParallelEmbedding
DeepseekModel o-- RMSNorm

class DeepseekDecoderLayer {
  +forward(positions, hidden_states, residual)
  -self_attn: DeepseekAttention
  -mlp: DeepseekMLP or DeepseekMoE
  -input_layernorm: RMSNorm
  -post_attention_layernorm: RMSNorm
}
DeepseekDecoderLayer *-- DeepseekAttention
DeepseekDecoderLayer *-- DeepseekMLP
DeepseekDecoderLayer *-- DeepseekMoE
DeepseekDecoderLayer o-- RMSNorm

class DeepseekAttention {
  +forward(positions, hidden_states)
  -qkv_proj: QKVParallelLinear
  -o_proj: RowParallelLinear
  -rotary_emb
  -attn: Attention
}
DeepseekAttention *-- QKVParallelLinear
DeepseekAttention *-- RowParallelLinear
DeepseekAttention *-- Attention

class DeepseekMLP {
  +forward(x)
  -gate_up_proj: MergedColumnParallelLinear
  -down_proj: RowParallelLinear
  -act_fn: SiluAndMul
}
DeepseekMLP *-- MergedColumnParallelLinear
DeepseekMLP *-- RowParallelLinear
DeepseekMLP o-- SiluAndMul

class DeepseekMoE {
  +forward(hidden_states)
  -experts: nn.ModuleList[DeepseekMLP]
  -gate: ReplicatedLinear
  -shared_experts: DeepseekMLP
}
DeepseekMoE o-- DeepseekMLP
DeepseekMoE o-- ReplicatedLinear

class ParallelLMHead
class LogitsProcessor

class DeepseekForCausalLM {
  +forward(...); +compute_logits(...); +load_weights(...)
  -model: DeepseekModel
  -lm_head: ParallelLMHead
  -logits_processor: LogitsProcessor
}
DeepseekForCausalLM o-- DeepseekModel
DeepseekForCausalLM o-- ParallelLMHead
DeepseekForCausalLM o-- LogitsProcessor

%% ========================= Serving =========================
class OpenAIServing {
  +engine_client: EngineClient
  +model_config: ModelConfig
  +models: OpenAIServingModels
  +request_logger: Optional[RequestLogger]
  +handle(ctx)
  +_preprocess(ctx); +_build_response(ctx)
  +_validate_request(ctx)
  +_create_pooling_params(ctx)
  +_prepare_generators(ctx)
  +_collect_batch(ctx)
  +create_error_response(...); +create_streaming_error_response(...)
  +_check_model(...); +_maybe_get_adapters(...)
  +_get_message_types(...); +_normalize_prompt_text_to_input(...); +_normalize_prompt_tokens_to_input(...)
  +_validate_input(...)
  +_tokenize_prompt_input_async(...); +_tokenize_prompt_inputs_async(...); +_tokenize_prompt_input_or_inputs_async(...)
  +_preprocess_completion(...); +_preprocess_chat(...); +_generate_with_builtin_tools(...)
  +_load_prompt_embeds(...); +_log_inputs(...); +_get_trace_headers(...)
  +_is_model_supported(...); +_get_model_name(...)
}
OpenAIServing o-- EngineClient
OpenAIServing o-- ModelConfig
OpenAIServing o-- OpenAIServingModels
OpenAIServing o-- RequestLogger

class EmbeddingMixin { +_preprocess(...) +_build_response(...) }
OpenAIServing <|-- EmbeddingMixin

class OpenAIServingEmbedding {
  +request_id_prefix: str
  +create_embedding(...)
  +_validate_request(...)
  +_create_pooling_params(...)
}
EmbeddingMixin <|-- OpenAIServingEmbedding
OpenAIServingEmbedding --> EmbeddingServeContext : uses
OpenAIServingEmbedding --> EmbeddingRequest : uses
OpenAIServingEmbedding --> EmbeddingResponse : returns

%% ========================= Platform / Backend =========================
class Platform {
  +_enum: PlatformEnum
  +device_name: str
  +device_type: str
  +dispatch_key: str
  +ray_device_key: str
  +device_control_env_var: str
  +simple_compile_backend: str
  +dist_backend: str
  +supported_quantization: list[str]
  +additional_env_vars: list[str]
  +supported_dtypes: list[torch.dtype]
  +is_cuda(): bool
  +is_rocm(): bool
  +is_tpu(): bool
  +is_xpu(): bool
  +is_cpu(): bool
  +is_neuron(): bool
  +is_out_of_tree(): bool
  +get_max_output_tokens(prompt_len: int): int
  +is_cuda_alike(): bool
  +is_sleep_mode_available(): bool
  +device_id_to_physical_device_id(device_id: int)
  +get_vit_attn_backend(support_fa: bool): _Backend
  +get_attn_backend_cls(selected_backend: _Backend, head_size: int, dtype: torch.dtype, kv_cache_dtype: Optional[str], block_size: int, use_v1: bool, use_mla: bool): str
  +get_device_capability(device_id: int): Optional[DeviceCapability]
  +has_device_capability(capability: Union[tuple[int, int], int], device_id: int): bool
  +is_device_capability(capability: Union[tuple[int, int], int], device_id: int): bool
  +get_device_name(device_id: int): str
  +get_device_uuid(device_id: int): str
  +get_device_total_memory(device_id: int): int
  +is_async_output_supported(enforce_eager: Optional[bool]): bool
  +inference_mode()
  +seed_everything(seed: Optional[int])
  +set_device(device: torch.device)
  +pre_register_and_update(parser: Optional[FlexibleArgumentParser])
  +check_and_update_config(vllm_config: VllmConfig)
  +verify_model_arch(model_arch: str)
  +verify_quantization(quant: str)
  +get_cpu_architecture(): CpuArchEnum
  +is_pin_memory_available(): bool
  +get_current_memory_usage(device: Optional[torch.types.Device]): float
  +get_punica_wrapper(): str
  +get_infinity_values(dtype: torch.dtype): tuple[float, float]
  +can_update_inplace(): bool
  +get_lora_vocab_padding_size(): int
  +get_device_communicator_cls(): str
  +supports_mx(): bool
  +supports_fp8(): bool
  +is_fp8_fnuz(): bool
  +fp8_dtype(): torch.dtype
  +use_all_gather(): bool
  +supports_v1(model_config: ModelConfig): bool
  +default_v1(model_config: ModelConfig): bool
  +use_custom_allreduce(): bool
  +validate_request(prompt: PromptType, params: Union[SamplingParams, PoolingParams], processed_inputs: ProcessorInputs)
  +__getattr__(key: str)
  +get_cu_count(device_id: int): int
  +get_piecewise_backend_cls(): str
  +stateless_init_device_torch_dist_pg(backend: str, prefix_store: PrefixStore, group_rank: int, group_size: int, timeout: timedelta): ProcessGroup
  +is_kv_cache_dtype_supported(kv_cache_dtype: str): bool
}
class UnspecifiedPlatform { +_enum: PlatformEnum +device_type: str }
Platform <|-- UnspecifiedPlatform

class CudaPlatformBase {
  +_enum: PlatformEnum
  +device_name: str
  +device_type: str
  +dispatch_key: str
  +ray_device_key: str
  +dist_backend: str
  +device_control_env_var: str
  +supported_dtypes: list[torch.dtype]
  +set_device(device: torch.device): None
  +get_device_capability(device_id: int): Optional[DeviceCapability]
  +get_device_name(device_id: int): str
  +get_device_total_memory(device_id: int): int
  +is_async_output_supported(enforce_eager: Optional[bool]): bool
  +is_fully_connected(device_ids: list[int]): bool
  +log_warnings()
  +check_and_update_config(vllm_config: VllmConfig): None
  +get_current_memory_usage(device: Optional[torch.types.Device]): float
  +get_vit_attn_backend(support_fa: bool): _Backend
  +get_attn_backend_cls(selected_backend, head_size, dtype, kv_cache_dtype, block_size, use_v1, use_mla): str
  +get_punica_wrapper(): str
  +get_device_communicator_cls(): str
  +supports_fp8(): bool
  +supports_v1(model_config: ModelConfig): bool
  +use_custom_allreduce(): bool
  +get_piecewise_backend_cls(): str
  +stateless_init_device_torch_dist_pg(backend, prefix_store, group_rank, group_size, timeout): ProcessGroup
  +device_count(): int
  +is_kv_cache_dtype_supported(kv_cache_dtype: str): bool
}
Platform <|-- CudaPlatformBase

class NvmlCudaPlatform {
  +get_device_capability(device_id: int): Optional[DeviceCapability]
  +has_device_capability(capability, device_id: int): bool
  +get_device_name(device_id: int): str
  +get_device_uuid(device_id: int): str
  +get_device_total_memory(device_id: int): int
  +is_fully_connected(physical_device_ids: list[int]): bool
  +_get_physical_device_name(device_id: int): str
  +log_warnings()
}
CudaPlatformBase <|-- NvmlCudaPlatform

class NonNvmlCudaPlatform {
  +get_device_capability(device_id: int): DeviceCapability
  +get_device_name(device_id: int): str
  +get_device_total_memory(device_id: int): int
  +is_fully_connected(physical_device_ids: list[int]): bool
}
CudaPlatformBase <|-- NonNvmlCudaPlatform

%% ---- platform loader / globals ----
class builtin_platform_plugins {
  +tpu: tpu_platform_plugin
  +cuda: cuda_platform_plugin
  +rocm: rocm_platform_plugin
  +xpu: xpu_platform_plugin
  +cpu: cpu_platform_plugin
  +neuron: neuron_platform_plugin
}
class platform_plugins { +load_plugins_by_group }
class _current_platform { +Platform instance }

%% ========================= Utilities & Misc =========================
class APIServerProcessManager {
  +__init__(target_server_fn, listen_address, sock, args, num_servers, input_addresses, output_addresses, stats_update_address)
  +close()
}
class ConstantList {
  +__init__(x: list[T])
  +append(item); +extend(item); +insert(item); +pop(item); +remove(item); +clear()
  +index(item, start, stop)
  +__getitem__(item); +__setitem__(item, value); +__delitem__(item)
  +__iter__(); +__contains__(item); +__len__(); +__repr__()
}
Sequence <|-- ConstantList

class MessageQueue
class VocabParallelEmbedding
class RMSNorm
class Attention
class SiluAndMul
class TokenizerGroup
class EngineClient
class RequestLogger
class OpenAIServingModels
class SequenceGroupMetadata
class SequenceGroupMetadataDelta
class SamplerOutput
class TensorizerConfig
class KVCacheManager
class EncoderCacheManager
class StructuredOutputManager
class KVConnectorBase_V1
class MultiModalRegistry
class VllmConfig
class ModelRunnerBase
class GPUModelRunnerBase
class ObservabilityConfig
class QuantizationConfig
class MemorySnapshot
class ExecuteModelRequest
class IntermediateTensors
class ParallelConfig
class EmbeddingServeContext
class EmbeddingRequest
class EmbeddingResponse
class FlexibleArgumentParser
class PromptType
class SamplingParams
class PoolingParams
class ProcessorInputs
class PrefixStore
class ProcessGroup
class CuMemAllocator
class logger
class gc
class os
class torch
class contextlib
class random
class np
class platform
class sys
class envs
class pynvml

%% ========================= High-level Cross-Module Links =========================
BlockSpaceManager --> CpuGpuBlockAllocator : uses
CpuGpuBlockAllocator --> BlockAllocator : uses
BlockAllocator --> Block : allocates
BlockTable --> Block : manages
BlockTable --> CpuGpuBlockAllocator : uses
BlockTable --> BlockTable : forks
CpuGpuBlockAllocator --> NullBlock : allocate_or_get_null_block
CpuGpuBlockAllocator --> Block : allocate_mutable_block, allocate_immutable_block

Scheduler --> KVCacheManager
Scheduler --> EncoderCacheManager
Scheduler --> StructuredOutputManager
Scheduler --> KVConnectorBase_V1
Scheduler --> MultiModalRegistry

MultiprocExecutor --> WorkerProc : manages
MultiprocExecutor --> WorkerProcHandle : manages
MultiprocExecutor --> UnreadyWorkerProcHandle : manages

RayDistributedExecutor --> KVOutputAggregator : uses
RayDistributedExecutor --> FutureWrapper : returns
