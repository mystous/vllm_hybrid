classDiagram
    %% worker_base.py

    class WorkerBase {
        +vllm_config: VllmConfig
        +model_config
        +cache_config
        +lora_config
        +load_config
        +parallel_config
        +scheduler_config
        +device_config
        +speculative_config
        +observability_config
        +kv_transfer_config
        +compilation_config
        +current_platform
        +init_device()
        +initialize_cache(num_gpu_blocks, num_cpu_blocks)
        +get_model()
        +load_model()
        +execute_model(execute_model_req)
        +start_worker_execution_loop()
        +determine_num_available_blocks()
        +get_cache_block_size_bytes()
        +add_lora(lora_request)
        +remove_lora(lora_id)
        +pin_lora(lora_id)
        +list_loras()
        +vocab_size
    }

    class DelegateWorkerBase {
        +worker: WorkerBase
        +init_device()
        +determine_num_available_blocks()
        +initialize_cache(num_gpu_blocks, num_cpu_blocks)
        +load_model()
        +get_model()
        +execute_model(execute_model_req)
        +get_cache_block_size_bytes()
        +add_lora(lora_request)
        +remove_lora(lora_id)
        +pin_lora(lora_id)
        +list_loras()
        +__getattr__(attr)
    }
    DelegateWorkerBase --|> WorkerBase

    class LoRANotSupportedWorkerBase {
        +add_lora(lora_request)
        +remove_lora(lora_id)
        +pin_lora(lora_id)
        +list_loras()
    }
    LoRANotSupportedWorkerBase --|> WorkerBase

    class WorkerInput {
        +num_seq_groups: Optional[int]
        +blocks_to_swap_in: Optional[torch.Tensor]
        +blocks_to_swap_out: Optional[torch.Tensor]
        +blocks_to_copy: Optional[torch.Tensor]
        +virtual_engine: int
        +num_steps: int
        +from_broadcasted_tensor_dict(tensor_dict)
        +as_broadcastable_tensor_dict()
    }

    class LocalOrDistributedWorkerBase {
        +is_driver_worker: bool
        +model_runner: ModelRunnerBase
        +observability_config: Optional[ObservabilityConfig]
        +do_metadata_broadcast
        +kv_cache
        +prepare_worker_input(execute_model_req)
        +execute_worker(worker_input)
        +prepare_input(execute_model_req)
        +get_model()
        +execute_model(execute_model_req)
        +_execute_model_spmd(execute_model_req, intermediate_tensors)
    }
    LocalOrDistributedWorkerBase --|> WorkerBase

    class WorkerWrapperBase {
        +rpc_rank: int
        +worker: Optional[WorkerBase]
        +vllm_config: Optional[VllmConfig]
        +adjust_rank(rank_mapping)
        +update_environment_variables(envs_list)
        +init_worker(all_kwargs)
        +initialize_from_config(kv_cache_configs)
        +init_device()
        +execute_method(method, *args, **kwargs)
        +__getattr__(attr)
    }

    %% worker.py

    class Worker {
        +local_rank: int
        +rank: int
        +distributed_init_method: str
        +is_driver_worker: bool
        +model_runner: GPUModelRunnerBase
        +cache_engine: List[CacheEngine]
        +gpu_cache: Optional[List[List[torch.Tensor]]]
        +_seq_group_metadata_cache: Dict[str, SequenceGroupMetadata]
        +_sleep_saved_buffers: Dict[str, torch.Tensor]
        +profiler
        +start_profile()
        +stop_profile()
        +sleep(level)
        +wake_up(tags)
        +init_device()
        +load_model()
        +save_sharded_state(path, pattern, max_size)
        +save_tensorized_model(tensorizer_config)
        +determine_num_available_blocks()
        +_assert_memory_footprint_increased_during_profiling()
        +initialize_cache(num_gpu_blocks, num_cpu_blocks)
        +_init_cache_engine()
        +_warm_up_model()
        +do_metadata_broadcast
        +kv_cache
        +prepare_worker_input(execute_model_req)
        +execute_worker(worker_input)
        +_get_cached_seq_group_metadata(seq_group_metadata_list, finished_request_ids)
        +_execute_model_spmd(execute_model_req, intermediate_tensors)
        +add_lora(lora_request)
        +remove_lora(lora_id)
        +pin_lora(lora_id)
        +list_loras()
        +max_model_len
        +vocab_size
        +get_cache_block_size_bytes()
    }
    Worker --|> LocalOrDistributedWorkerBase

    %% 관계
    Worker "1" o-- "1" GPUModelRunnerBase
    Worker "1" o-- "1..*" CacheEngine
    Worker "1" o-- "1" WorkerInput
    Worker "1" o-- "1" SequenceGroupMetadata
    Worker "1" o-- "1" LoRARequest

    LocalOrDistributedWorkerBase "1" o-- "1" ModelRunnerBase
    WorkerBase "1" o-- "1" VllmConfig
    WorkerInput "1" o-- "1" torch.Tensor

    WorkerWrapperBase "1" o-- "1" WorkerBase
