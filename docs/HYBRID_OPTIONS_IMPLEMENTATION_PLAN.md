# CPU+GPU í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” êµ¬í˜„ ê³„íšì„œ

> **ëª©í‘œ**: GPU onlyë³´ë‹¤ ë¹ ë¥¸ CPU+GPU í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ 
> **ì‘ì„±ì¼**: 2026-02-03

---

## ê°œìš”

### ë‘ ê°€ì§€ ì˜µì…˜

| ì˜µì…˜ | êµ¬ì„± | ëŒ€ìƒ í™˜ê²½ |
|------|------|----------|
| **Option A** | MoE Offload + N-gram + Disaggregated | MoE ëª¨ë¸ (DeepSeek ë“±) |
| **Option B** | APEX ìŠ¤ì¼€ì¤„ë§ | Dense ëª¨ë¸, ì œí•œëœ GPU |

### ì‹¤í–‰ ì˜ˆì‹œ

```bash
# Option A: MoE ìµœì í™” (DeepSeek R1 ê¶Œì¥)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --hybrid-mode moe-hybrid \
  --moe-cpu-offload \
  --ngram-spec-decode \
  --disaggregated-prefill

# Option B: Parallel Batch (APEX ìŠ¤ì¼€ì¤„ë§) - êµ¬í˜„ ì™„ë£Œ
vllm serve meta-llama/Llama-3-70B \
  --hybrid-mode parallel-batch \
  --hybrid-cpu-ratio 0.2 \
  --hybrid-cpu-threads 112 \
  --hybrid-numa-aware

# Option B: 112 ì½”ì–´ NUMA ì „ì²´ í™œìš© (ê¶Œì¥)
numactl --interleave=all vllm serve meta-llama/Llama-3-70B \
  --hybrid-mode parallel-batch \
  --hybrid-cpu-threads 112 \
  --hybrid-cpu-dtype bfloat16
```

### êµ¬í˜„ ìƒíƒœ (2026-02-03)

| ì˜µì…˜ | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| **parallel-batch** | âœ… êµ¬í˜„ ì™„ë£Œ | IPEX + NUMA + AMX ìµœì í™” |
| **moe-hybrid** | ğŸ”² Dummy | ì¶”í›„ êµ¬í˜„ ì˜ˆì • |

---

## ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [Option A ìƒì„¸ ì„¤ê³„](#2-option-a-ìƒì„¸-ì„¤ê³„)
3. [Option B ìƒì„¸ ì„¤ê³„](#3-option-b-ìƒì„¸-ì„¤ê³„)
4. [ê³µí†µ ì¸í”„ë¼](#4-ê³µí†µ-ì¸í”„ë¼)
5. [êµ¬í˜„ ë¡œë“œë§µ](#5-êµ¬í˜„-ë¡œë“œë§µ)
6. [íŒŒì¼ êµ¬ì¡°](#6-íŒŒì¼-êµ¬ì¡°)
7. [API ì„¤ê³„](#7-api-ì„¤ê³„)
8. [í…ŒìŠ¤íŠ¸ ê³„íš](#8-í…ŒìŠ¤íŠ¸-ê³„íš)

---

## 1. ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 Option A ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Option A ì•„í‚¤í…ì²˜                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    [3] Disaggregated Serving                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚   Prefill Node    â”‚   KV    â”‚      Decode Node          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   (GPU/CPU Pool)  â”‚ â”€â”€â”€â”€â”€â†’  â”‚      (GPU + CPU)          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                   â”‚ Cache   â”‚                           â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ê¸´ í”„ë¡¬í”„íŠ¸    â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ë°°ì¹˜ ì²˜ë¦¬      â”‚         â”‚  â”‚ [1] MoE Offload     â”‚  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ë†’ì€ ì²˜ë¦¬ëŸ‰    â”‚         â”‚  â”‚ GPU: Attention      â”‚  â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚ CPU: Experts        â”‚  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚            +              â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â”‚ [2] N-gram Lookup   â”‚  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â”‚ CPU: íŒ¨í„´ ë§¤ì¹­      â”‚  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â”‚ GPU: ê²€ì¦           â”‚  â”‚    â”‚   â”‚
â”‚  â”‚                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚   â”‚
â”‚  â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Option B ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Option B ì•„í‚¤í…ì²˜                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      APEX Scheduler                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚              Profiler + Dynamic Partitioner                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - ë°°ì¹˜ë³„ ìµœì  CPU/GPU ë¹„ìœ¨ ê²°ì •                           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - ëŸ°íƒ€ì„ ì¡°ì •                                             â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                              â†“                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚    GPU Worker       â”‚    â”‚       CPU Worker            â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Batch A, C, E â”‚  â”‚    â”‚  â”‚ Batch B, D, F         â”‚  â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ (ì „ì²´ ëª¨ë¸)   â”‚  â”‚    â”‚  â”‚ (ì „ì²´ ëª¨ë¸, INT8)     â”‚  â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  ì²˜ë¦¬ëŸ‰: 80%        â”‚    â”‚  ì²˜ë¦¬ëŸ‰: 20%                â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â”‚                              â†“                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚                    Result Merger                           â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Option A ìƒì„¸ ì„¤ê³„

### 2.1 ì»´í¬ë„ŒíŠ¸ 1: MoE Expert Offload

#### 2.1.1 ê°œë…

```
MoE ë ˆì´ì–´ êµ¬ì¡°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input â†’ Router â†’ [Expert 0] [Expert 1] ... [Expert N] â†’ Output
â”‚                        â†‘          â†‘              â†‘
â”‚                    í™œì„±í™”      ë¹„í™œì„±í™”       ë¹„í™œì„±í™”
â”‚                   (GPU ì‹¤í–‰)  (CPU ëŒ€ê¸°)     (CPU ëŒ€ê¸°)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- Routerê°€ Top-K expert ì„ íƒ (ë³´í†µ K=2 ë˜ëŠ” K=8)
- ì„ íƒëœ expertë§Œ ê³„ì‚° í•„ìš”
- ë‚˜ë¨¸ì§€ëŠ” ë©”ëª¨ë¦¬ë§Œ ì°¨ì§€
```

#### 2.1.2 êµ¬í˜„ ì „ëµ

```python
# vllm/model_executor/layers/moe/expert_offload.py

class ExpertOffloadManager:
    """MoE Expert CPU-GPU ì˜¤í”„ë¡œë“œ ê´€ë¦¬ì"""

    def __init__(
        self,
        num_experts: int,
        expert_size: int,
        num_gpu_experts: int,  # GPUì— ìƒì£¼í•  expert ìˆ˜
        cpu_dtype: torch.dtype = torch.bfloat16,
    ):
        self.num_experts = num_experts
        self.num_gpu_experts = num_gpu_experts

        # GPUì— ìƒì£¼í•  "hot" experts (ìì£¼ ì‚¬ìš©ë˜ëŠ” ê²ƒ)
        self.gpu_experts: Dict[int, nn.Module] = {}

        # CPUì— ëŒ€ê¸°í•  experts
        self.cpu_experts: Dict[int, nn.Module] = {}

        # Expert ì‚¬ìš© í†µê³„ (LRU ìºì‹œìš©)
        self.expert_usage_count: Dict[int, int] = defaultdict(int)

        # CPU-GPU ì „ì†¡ ìŠ¤íŠ¸ë¦¼
        self.transfer_stream = torch.cuda.Stream()

    def route_and_compute(
        self,
        hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
    ) -> torch.Tensor:
        """ë¼ìš°íŒ… í›„ expert ê³„ì‚°"""

        # 1. ë¼ìš°í„°ë¡œ expert ì„ íƒ
        routing_weights, selected_experts = self._compute_routing(router_logits)

        # 2. ì„ íƒëœ expert ë¶„ë¥˜
        gpu_experts, cpu_experts = self._classify_experts(selected_experts)

        # 3. GPU expert ê³„ì‚° (ì¦‰ì‹œ)
        gpu_output = self._compute_gpu_experts(
            hidden_states, routing_weights, gpu_experts
        )

        # 4. CPU expert ê³„ì‚° (ë³‘ë ¬)
        cpu_output = self._compute_cpu_experts(
            hidden_states, routing_weights, cpu_experts
        )

        # 5. ê²°ê³¼ í•©ì‚°
        return gpu_output + cpu_output.to(hidden_states.device)

    def _compute_cpu_experts(
        self,
        hidden_states: torch.Tensor,
        routing_weights: torch.Tensor,
        expert_indices: List[int],
    ) -> torch.Tensor:
        """CPUì—ì„œ expert ê³„ì‚° (AVX-512 ìµœì í™”)"""

        # GPU â†’ CPU ì „ì†¡
        hidden_cpu = hidden_states.to('cpu', non_blocking=True)

        # CPUì—ì„œ ê³„ì‚° (ë³‘ë ¬)
        outputs = []
        with ThreadPoolExecutor(max_workers=len(expert_indices)) as executor:
            futures = []
            for idx in expert_indices:
                expert = self.cpu_experts[idx]
                weight = routing_weights[:, idx]
                futures.append(
                    executor.submit(self._run_expert_cpu, expert, hidden_cpu, weight)
                )

            for future in futures:
                outputs.append(future.result())

        # í•©ì‚°
        result = torch.stack(outputs).sum(dim=0)
        return result

    def _run_expert_cpu(
        self,
        expert: nn.Module,
        hidden: torch.Tensor,
        weight: torch.Tensor,
    ) -> torch.Tensor:
        """ë‹¨ì¼ expert CPU ì‹¤í–‰ (AVX-512 VNNI ìµœì í™”)"""

        with torch.no_grad():
            # INT8 ì–‘ìí™”ëœ expert ì‚¬ìš© ì‹œ
            if hasattr(expert, 'int8_forward'):
                output = expert.int8_forward(hidden)
            else:
                output = expert(hidden)

            return output * weight.unsqueeze(-1)

    def update_expert_cache(self, selected_experts: torch.Tensor):
        """Expert ìºì‹œ ì—…ë°ì´íŠ¸ (LRU ê¸°ë°˜)"""

        # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        unique_experts = selected_experts.unique().tolist()
        for idx in unique_experts:
            self.expert_usage_count[idx] += 1

        # ì£¼ê¸°ì ìœ¼ë¡œ GPU expert êµì²´
        if self._should_swap():
            self._swap_experts()

    def _swap_experts(self):
        """ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ expert êµì²´"""

        # ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” expertë¥¼ GPUë¡œ
        sorted_experts = sorted(
            self.expert_usage_count.items(),
            key=lambda x: x[1],
            reverse=True
        )

        new_gpu_experts = [idx for idx, _ in sorted_experts[:self.num_gpu_experts]]

        # ë¹„ë™ê¸° ì „ì†¡
        with torch.cuda.stream(self.transfer_stream):
            for idx in new_gpu_experts:
                if idx not in self.gpu_experts:
                    # CPU â†’ GPU
                    self.gpu_experts[idx] = self.cpu_experts.pop(idx).cuda()

            for idx in list(self.gpu_experts.keys()):
                if idx not in new_gpu_experts:
                    # GPU â†’ CPU
                    self.cpu_experts[idx] = self.gpu_experts.pop(idx).cpu()
```

#### 2.1.3 MoE ë ˆì´ì–´ í†µí•©

```python
# vllm/model_executor/layers/moe/fused_moe_offload.py

class FusedMoEWithOffload(nn.Module):
    """CPU ì˜¤í”„ë¡œë“œë¥¼ ì§€ì›í•˜ëŠ” Fused MoE ë ˆì´ì–´"""

    def __init__(
        self,
        num_experts: int,
        top_k: int,
        hidden_size: int,
        intermediate_size: int,
        offload_config: Optional[ExpertOffloadConfig] = None,
    ):
        super().__init__()

        self.num_experts = num_experts
        self.top_k = top_k

        # ë¼ìš°í„°
        self.gate = nn.Linear(hidden_size, num_experts, bias=False)

        # Experts
        self.experts = nn.ModuleList([
            MoEExpert(hidden_size, intermediate_size)
            for _ in range(num_experts)
        ])

        # ì˜¤í”„ë¡œë“œ ê´€ë¦¬ì
        if offload_config and offload_config.enabled:
            self.offload_manager = ExpertOffloadManager(
                num_experts=num_experts,
                expert_size=intermediate_size,
                num_gpu_experts=offload_config.num_gpu_experts,
            )
            self._setup_offload()
        else:
            self.offload_manager = None

    def _setup_offload(self):
        """Expert ì˜¤í”„ë¡œë“œ ì´ˆê¸° ì„¤ì •"""

        # ì²˜ìŒì—ëŠ” ê· ë“± ë¶„ë°°
        num_gpu = self.offload_manager.num_gpu_experts

        for i, expert in enumerate(self.experts):
            if i < num_gpu:
                self.offload_manager.gpu_experts[i] = expert.cuda()
            else:
                # INT8 ì–‘ìí™” í›„ CPUë¡œ
                expert_int8 = quantize_expert_to_int8(expert)
                self.offload_manager.cpu_experts[i] = expert_int8.cpu()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """Forward with optional offload"""

        # ë¼ìš°íŒ…
        router_logits = self.gate(hidden_states)

        if self.offload_manager:
            # ì˜¤í”„ë¡œë“œ ëª¨ë“œ
            return self.offload_manager.route_and_compute(
                hidden_states, router_logits
            )
        else:
            # ì¼ë°˜ ëª¨ë“œ
            return self._standard_moe_forward(hidden_states, router_logits)
```

### 2.2 ì»´í¬ë„ŒíŠ¸ 2: N-gram Lookahead Decoding

#### 2.2.1 ê°œë…

```
N-gram Lookahead:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CPU: ì´ì „ ì¶œë ¥ì—ì„œ N-gram íŒ¨í„´ ë§¤ì¹­                          â”‚
â”‚     "The quick brown" â†’ ê³¼ê±°ì— "The quick brown fox" ì¶œë ¥í•œ ì  ìˆìŒ
â”‚     â†’ ["fox", "jumps", "over"] ì¶”ì¸¡                              â”‚
â”‚                                                                  â”‚
â”‚  2. GPU: ì¶”ì¸¡ í† í° ê²€ì¦ (í•œ ë²ˆì˜ forward)                        â”‚
â”‚     Input: "The quick brown" + ["fox", "jumps", "over"]          â”‚
â”‚     Output: [âœ“ fox] [âœ“ jumps] [âœ— over â†’ "the"]                  â”‚
â”‚     â†’ 2ê°œ í† í° ì¦‰ì‹œ ì±„íƒ!                                        â”‚
â”‚                                                                  â”‚
â”‚  íš¨ê³¼: 3ê°œ í† í°ì„ 1ë²ˆì˜ forwardë¡œ ì²˜ë¦¬ (vs 3ë²ˆ)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.2 êµ¬í˜„

```python
# vllm/spec_decode/ngram_proposer.py

import numpy as np
from collections import defaultdict
from typing import List, Tuple, Optional
import threading

class NGramProposer:
    """N-gram ê¸°ë°˜ ì¶”ì¸¡ í† í° ì œì•ˆì (CPUì—ì„œ ì‹¤í–‰)"""

    def __init__(
        self,
        n: int = 3,                    # N-gram í¬ê¸°
        num_speculative_tokens: int = 5,  # ì¶”ì¸¡í•  í† í° ìˆ˜
        min_frequency: int = 2,        # ìµœì†Œ ì¶œí˜„ ë¹ˆë„
    ):
        self.n = n
        self.num_speculative_tokens = num_speculative_tokens
        self.min_frequency = min_frequency

        # N-gram ì €ì¥ì†Œ: (n-1)-gram â†’ {next_token: count}
        self.ngram_store: Dict[Tuple[int, ...], Dict[int, int]] = defaultdict(
            lambda: defaultdict(int)
        )

        # ë½ (ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „)
        self.lock = threading.RLock()

        # í†µê³„
        self.total_proposals = 0
        self.accepted_proposals = 0

    def update(self, token_ids: List[int]):
        """ì¶œë ¥ í† í°ìœ¼ë¡œ N-gram ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ)"""

        with self.lock:
            for i in range(len(token_ids) - self.n + 1):
                prefix = tuple(token_ids[i:i + self.n - 1])
                next_token = token_ids[i + self.n - 1]
                self.ngram_store[prefix][next_token] += 1

    def propose(
        self,
        context_tokens: List[int],
    ) -> Tuple[List[int], List[float]]:
        """N-gram ê¸°ë°˜ ì¶”ì¸¡ í† í° ì œì•ˆ"""

        proposals = []
        confidences = []

        current_context = list(context_tokens)

        with self.lock:
            for _ in range(self.num_speculative_tokens):
                # (n-1)-gram prefix ì¶”ì¶œ
                if len(current_context) >= self.n - 1:
                    prefix = tuple(current_context[-(self.n - 1):])
                else:
                    prefix = tuple(current_context)

                # ë‹¤ìŒ í† í° ì˜ˆì¸¡
                if prefix in self.ngram_store:
                    candidates = self.ngram_store[prefix]

                    # ë¹ˆë„ ê¸°ë°˜ ì •ë ¬
                    sorted_candidates = sorted(
                        candidates.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )

                    if sorted_candidates and sorted_candidates[0][1] >= self.min_frequency:
                        next_token = sorted_candidates[0][0]
                        total_count = sum(c for _, c in sorted_candidates)
                        confidence = sorted_candidates[0][1] / total_count

                        proposals.append(next_token)
                        confidences.append(confidence)
                        current_context.append(next_token)
                        continue

                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                break

        self.total_proposals += len(proposals)
        return proposals, confidences

    def record_acceptance(self, num_accepted: int):
        """ì±„íƒëœ í† í° ìˆ˜ ê¸°ë¡ (í†µê³„ìš©)"""
        self.accepted_proposals += num_accepted

    @property
    def acceptance_rate(self) -> float:
        """ì±„íƒë¥ """
        if self.total_proposals == 0:
            return 0.0
        return self.accepted_proposals / self.total_proposals


class NGramLookaheadWorker:
    """N-gram Lookahead ì›Œì»¤ (CPU ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""

    def __init__(
        self,
        proposer: NGramProposer,
        tokenizer,
    ):
        self.proposer = proposer
        self.tokenizer = tokenizer

        # ë¹„ë™ê¸° ì—…ë°ì´íŠ¸ í
        self.update_queue = queue.Queue()

        # ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ
        self.update_thread = threading.Thread(
            target=self._background_update,
            daemon=True
        )
        self.update_thread.start()

    def _background_update(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ N-gram ì—…ë°ì´íŠ¸"""
        while True:
            try:
                token_ids = self.update_queue.get(timeout=1.0)
                self.proposer.update(token_ids)
            except queue.Empty:
                continue

    def get_proposals(
        self,
        context_token_ids: List[int],
    ) -> List[int]:
        """ì¶”ì¸¡ í† í° ë°˜í™˜ (CPUì—ì„œ ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ë¡œ ì‹¤í–‰)"""

        proposals, _ = self.proposer.propose(context_token_ids)
        return proposals

    def submit_output(self, output_token_ids: List[int]):
        """ì¶œë ¥ í† í°ì„ ì—…ë°ì´íŠ¸ íì— ì¶”ê°€"""
        self.update_queue.put(output_token_ids)
```

#### 2.2.3 vLLM Speculative Decoding í†µí•©

```python
# vllm/spec_decode/ngram_spec_worker.py

from vllm.spec_decode.interfaces import SpeculativeProposer
from vllm.spec_decode.ngram_proposer import NGramProposer, NGramLookaheadWorker

class NGramSpeculativeWorker(SpeculativeProposer):
    """N-gram ê¸°ë°˜ Speculative Decoding ì›Œì»¤"""

    def __init__(
        self,
        proposer_config: NGramProposerConfig,
        tokenizer,
    ):
        self.proposer = NGramProposer(
            n=proposer_config.n,
            num_speculative_tokens=proposer_config.num_speculative_tokens,
        )
        self.worker = NGramLookaheadWorker(self.proposer, tokenizer)

    def get_spec_proposals(
        self,
        execute_model_req: ExecuteModelRequest,
    ) -> SpeculativeProposals:
        """ì¶”ì¸¡ í† í° ì œì•ˆ"""

        proposals = []

        for seq_group in execute_model_req.seq_group_metadata_list:
            seq_data = seq_group.seq_data
            context_tokens = list(seq_data.get_token_ids())

            # CPUì—ì„œ N-gram ë§¤ì¹­ (ë§¤ìš° ë¹ ë¦„)
            proposed_tokens = self.worker.get_proposals(context_tokens)

            proposals.append(proposed_tokens)

        return SpeculativeProposals(
            proposal_token_ids=proposals,
            proposal_probs=None,  # N-gramì€ í™•ë¥  ì—†ìŒ
            proposal_lens=[len(p) for p in proposals],
        )

    def update_from_output(
        self,
        execute_model_req: ExecuteModelRequest,
        sampler_output: SamplerOutput,
    ):
        """ì¶œë ¥ìœ¼ë¡œ N-gram í…Œì´ë¸” ì—…ë°ì´íŠ¸"""

        for seq_group, output in zip(
            execute_model_req.seq_group_metadata_list,
            sampler_output.outputs
        ):
            output_tokens = [o.token_id for o in output.samples]
            self.worker.submit_output(output_tokens)
```

### 2.3 ì»´í¬ë„ŒíŠ¸ 3: Disaggregated Serving

#### 2.3.1 ê°œë…

```
Disaggregated Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚  Prefill Pool                    Decode Pool                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ GPU 0        â”‚               â”‚ GPU 4                â”‚        â”‚
â”‚  â”‚ GPU 1        â”‚    KV Cache   â”‚ GPU 5                â”‚        â”‚
â”‚  â”‚ (ë˜ëŠ” CPU)   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚ GPU 6                â”‚        â”‚
â”‚  â”‚              â”‚    Transfer   â”‚ GPU 7                â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚        â†‘                              â†“                          â”‚
â”‚  ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬                ì§§ì€ ì‘ë‹µ ìƒì„±                   â”‚
â”‚  (ì²˜ë¦¬ëŸ‰ ìµœì í™”)                (ì§€ì—°ì‹œê°„ ìµœì í™”)                 â”‚
â”‚                                                                  â”‚
â”‚  íŠ¹ì„±:                          íŠ¹ì„±:                            â”‚
â”‚  - Compute bound               - Memory bound                   â”‚
â”‚  - ë°°ì¹˜ íš¨ìœ¨ì                  - ë°°ì¹˜ ë¹„íš¨ìœ¨ì                    â”‚
â”‚  - Latency tolerance ë†’ìŒ      - Latency critical               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.3.2 êµ¬í˜„

```python
# vllm/engine/disaggregated/prefill_node.py

class PrefillNode:
    """Prefill ì „ìš© ë…¸ë“œ"""

    def __init__(
        self,
        model_config: ModelConfig,
        device_config: DeviceConfig,  # GPU ë˜ëŠ” CPU
        kv_transfer_config: KVTransferConfig,
    ):
        self.model_config = model_config
        self.device = device_config.device

        # ëª¨ë¸ ë¡œë“œ (Prefill ìµœì í™”)
        self.model = self._load_model_for_prefill()

        # KV Cache ì „ì†¡ í´ë¼ì´ì–¸íŠ¸
        self.kv_sender = KVCacheSender(kv_transfer_config)

    def _load_model_for_prefill(self):
        """Prefill ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ"""

        model = load_model(self.model_config)

        if self.device == 'cpu':
            # CPU Prefill: INT8 ì–‘ìí™” + AVX-512
            model = quantize_for_cpu_prefill(model)
            model = model.to('cpu')
        else:
            # GPU Prefill: Flash Attention í™œì„±í™”
            model = optimize_for_gpu_prefill(model)
            model = model.cuda()

        return model

    async def run_prefill(
        self,
        request: PrefillRequest,
    ) -> PrefillResult:
        """Prefill ì‹¤í–‰ ë° KV Cache ì „ì†¡"""

        # 1. Prefill ì‹¤í–‰
        with torch.no_grad():
            hidden_states, kv_cache = self.model.prefill(
                input_ids=request.input_ids,
                attention_mask=request.attention_mask,
            )

        # 2. KV Cacheë¥¼ Decode ë…¸ë“œë¡œ ì „ì†¡
        kv_transfer_handle = await self.kv_sender.send_async(
            kv_cache=kv_cache,
            dest_node=request.decode_node_id,
            request_id=request.request_id,
        )

        return PrefillResult(
            request_id=request.request_id,
            kv_transfer_handle=kv_transfer_handle,
            num_tokens=request.input_ids.shape[1],
        )


# vllm/engine/disaggregated/decode_node.py

class DecodeNode:
    """Decode ì „ìš© ë…¸ë“œ (Option Aì˜ MoE Offload + N-gram í¬í•¨)"""

    def __init__(
        self,
        model_config: ModelConfig,
        moe_offload_config: Optional[ExpertOffloadConfig],
        ngram_config: Optional[NGramProposerConfig],
        kv_transfer_config: KVTransferConfig,
    ):
        self.model_config = model_config

        # ëª¨ë¸ ë¡œë“œ (MoE Offload ì ìš©)
        self.model = self._load_model_with_offload(moe_offload_config)

        # N-gram Proposer
        if ngram_config:
            self.ngram_worker = NGramSpeculativeWorker(ngram_config, self.tokenizer)
        else:
            self.ngram_worker = None

        # KV Cache ìˆ˜ì‹ 
        self.kv_receiver = KVCacheReceiver(kv_transfer_config)

    def _load_model_with_offload(self, offload_config):
        """MoE Offload ì ìš©ëœ ëª¨ë¸ ë¡œë“œ"""

        model = load_model(self.model_config)

        if offload_config and offload_config.enabled:
            # MoE ë ˆì´ì–´ì— ì˜¤í”„ë¡œë“œ ì ìš©
            for name, module in model.named_modules():
                if isinstance(module, MoELayer):
                    offloaded = FusedMoEWithOffload(
                        num_experts=module.num_experts,
                        top_k=module.top_k,
                        hidden_size=module.hidden_size,
                        intermediate_size=module.intermediate_size,
                        offload_config=offload_config,
                    )
                    # êµì²´
                    set_module_by_name(model, name, offloaded)

        return model.cuda()

    async def run_decode(
        self,
        request: DecodeRequest,
    ) -> DecodeResult:
        """Decode ì‹¤í–‰ (N-gram Speculative í¬í•¨)"""

        # 1. KV Cache ìˆ˜ì‹  ëŒ€ê¸°
        kv_cache = await self.kv_receiver.receive_async(request.request_id)

        # 2. Decode ë£¨í”„
        output_tokens = []

        while not self._should_stop(request, output_tokens):
            # N-gram ì¶”ì¸¡ (CPU)
            if self.ngram_worker:
                spec_tokens = self.ngram_worker.get_spec_proposals(
                    context_tokens=request.input_ids + output_tokens
                )
            else:
                spec_tokens = []

            # Forward (GPU, ì¶”ì¸¡ í† í° í¬í•¨)
            if spec_tokens:
                # Speculative forward
                logits = self.model.forward_speculative(
                    input_ids=output_tokens[-1:] + spec_tokens,
                    kv_cache=kv_cache,
                )

                # ê²€ì¦
                accepted, rejected_idx = self._verify_speculative(
                    logits, spec_tokens
                )
                output_tokens.extend(accepted)

                # N-gram ì—…ë°ì´íŠ¸
                self.ngram_worker.update_from_output(accepted)
            else:
                # ì¼ë°˜ forward
                logits = self.model.forward(
                    input_ids=output_tokens[-1:],
                    kv_cache=kv_cache,
                )
                next_token = self._sample(logits)
                output_tokens.append(next_token)

        return DecodeResult(
            request_id=request.request_id,
            output_tokens=output_tokens,
        )


# vllm/engine/disaggregated/coordinator.py

class DisaggregatedCoordinator:
    """Prefill/Decode ë…¸ë“œ ì¡°ìœ¨ì"""

    def __init__(
        self,
        prefill_nodes: List[PrefillNode],
        decode_nodes: List[DecodeNode],
    ):
        self.prefill_nodes = prefill_nodes
        self.decode_nodes = decode_nodes

        # ë¡œë“œ ë°¸ëŸ°ì„œ
        self.prefill_lb = LoadBalancer(prefill_nodes)
        self.decode_lb = LoadBalancer(decode_nodes)

    async def process_request(
        self,
        request: InferenceRequest,
    ) -> InferenceResponse:
        """ìš”ì²­ ì²˜ë¦¬"""

        # 1. Prefill ë…¸ë“œ ì„ íƒ ë° ì‹¤í–‰
        prefill_node = self.prefill_lb.select()
        prefill_result = await prefill_node.run_prefill(
            PrefillRequest(
                request_id=request.id,
                input_ids=request.input_ids,
                decode_node_id=self._select_decode_node().id,
            )
        )

        # 2. Decode ë…¸ë“œì—ì„œ ìƒì„±
        decode_node = self.decode_lb.select()
        decode_result = await decode_node.run_decode(
            DecodeRequest(
                request_id=request.id,
                input_ids=request.input_ids,
                max_tokens=request.max_tokens,
            )
        )

        return InferenceResponse(
            request_id=request.id,
            output_tokens=decode_result.output_tokens,
        )
```

---

## 3. Option B ìƒì„¸ ì„¤ê³„

### 3.1 APEX ìŠ¤ì¼€ì¤„ëŸ¬

#### 3.1.1 ê°œë…

```
APEX (Asynchronous Parallel EXecution):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  í”„ë¡œíŒŒì¼ë§ ë‹¨ê³„:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ GPU ì„±ëŠ¥ ì¸¡ì •: 70B ëª¨ë¸, batch=1 â†’ 16 tok/s              â”‚   â”‚
â”‚  â”‚ CPU ì„±ëŠ¥ ì¸¡ì •: 70B INT8 ëª¨ë¸, batch=1 â†’ 3 tok/s          â”‚   â”‚
â”‚  â”‚ ìµœì  ë¹„ìœ¨: GPU 84%, CPU 16%                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  ëŸ°íƒ€ì„:                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 10ê°œ ìš”ì²­ ë„ì°©                                            â”‚   â”‚
â”‚  â”‚ â†’ GPU: 8ê°œ ë°°ì¹˜ (84%)                                    â”‚   â”‚
â”‚  â”‚ â†’ CPU: 2ê°œ ë°°ì¹˜ (16%)                                    â”‚   â”‚
â”‚  â”‚ â†’ ë™ì‹œ ì‹¤í–‰ â†’ ì´ ì²˜ë¦¬ëŸ‰ = GPU + CPU                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.2 ìë™ í”„ë¡œíŒŒì¼ë§ ìƒì„¸ ì„¤ê³„

### 3.2.1 í˜„ì¬ êµ¬í˜„ ìƒíƒœ (2026-02-03)

| í•­ëª© | êµ¬í˜„ ë°©ì‹ | ìƒíƒœ |
|------|----------|------|
| GPU ì²˜ë¦¬ëŸ‰ | íœ´ë¦¬ìŠ¤í‹± (ê³ ì •ê°’ 100 tok/s) | âš ï¸ ì¶”ì •ê°’ |
| CPU ì²˜ë¦¬ëŸ‰ | ì‹¤ì œ ì¸¡ì • (ë”ë¯¸ ì…ë ¥ ì¶”ë¡ ) | âœ… êµ¬í˜„ë¨ |
| ë¹„ìœ¨ ê³„ì‚° | ì²˜ë¦¬ëŸ‰ ë¹„ë¡€ ë¶„ë°° | âœ… êµ¬í˜„ë¨ |

### 3.2.2 ë¹„ìœ¨ ê³„ì‚° ê³µì‹

```
ìµœì  ë¹„ìœ¨ ê³„ì‚°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚  1. ì²˜ë¦¬ëŸ‰ ì¸¡ì •                                                  â”‚
â”‚     T_gpu = GPU ì²˜ë¦¬ëŸ‰ (tok/s)                                  â”‚
â”‚     T_cpu = CPU ì²˜ë¦¬ëŸ‰ (tok/s)                                  â”‚
â”‚                                                                  â”‚
â”‚  2. ë¹„ìœ¨ ê³„ì‚° (ì²˜ë¦¬ëŸ‰ ë¹„ë¡€ ë¶„ë°°)                                 â”‚
â”‚     R_gpu = T_gpu / (T_gpu + T_cpu)                             â”‚
â”‚     R_cpu = T_cpu / (T_gpu + T_cpu)                             â”‚
â”‚                                                                  â”‚
â”‚  3. ì˜ˆì‹œ (H100 + Xeon 8480+)                                    â”‚
â”‚     T_gpu = 100 tok/s (ì¶”ì •)                                    â”‚
â”‚     T_cpu = 5 tok/s (ì¸¡ì •)                                      â”‚
â”‚     R_gpu = 100 / 105 = 95.2%                                   â”‚
â”‚     R_cpu = 5 / 105 = 4.8%                                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2.3 CPU ì²˜ë¦¬ëŸ‰ ì¸¡ì • ë°©ë²• (êµ¬í˜„ë¨)

```python
def _measure_cpu_throughput(self, num_batches: int = 5) -> float:
    """CPU ì²˜ë¦¬ëŸ‰ ì‹¤ì œ ì¸¡ì •."""

    # 1. ë”ë¯¸ ì…ë ¥ ìƒì„±
    dummy_input = torch.randint(0, 32000, (batch_size, seq_len), device='cpu')

    # 2. ì›Œë°ì—… (2íšŒ)
    for _ in range(2):
        with torch.no_grad():
            _ = self.cpu_worker.model(dummy_input)

    # 3. ì‹¤ì œ ì¸¡ì •
    start = time.perf_counter()
    for _ in range(num_batches):
        with torch.no_grad(), torch.cpu.amp.autocast(enabled=use_bf16):
            _ = self.cpu_worker.model(dummy_input)
    elapsed = time.perf_counter() - start

    # 4. ì²˜ë¦¬ëŸ‰ ê³„ì‚°
    total_tokens = num_batches * seq_len * batch_size
    throughput = total_tokens / elapsed  # tok/s

    return throughput
```

### 3.2.4 GPU ì²˜ë¦¬ëŸ‰ ì¸¡ì • (TODO - ê°œì„  í•„ìš”)

í˜„ì¬ GPU ì²˜ë¦¬ëŸ‰ì€ **íœ´ë¦¬ìŠ¤í‹± ì¶”ì •ê°’**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
def _measure_gpu_throughput(self, num_batches: int) -> float:
    """GPU ì²˜ë¦¬ëŸ‰ ì¸¡ì • - í˜„ì¬ëŠ” ì¶”ì •ê°’ ì‚¬ìš©."""

    # TODO: ì‹¤ì œ GPU executorë¡œ ì¸¡ì • êµ¬í˜„ í•„ìš”
    # í˜„ì¬ëŠ” H100 ê¸°ì¤€ ì¶”ì •ê°’ ì‚¬ìš©
    estimated_gpu_throughput = 100.0  # tok/s per sequence

    return estimated_gpu_throughput
```

**ê°œì„  ê³„íš:**

```python
def _measure_gpu_throughput(self, num_batches: int) -> float:
    """GPU ì²˜ë¦¬ëŸ‰ ì‹¤ì œ ì¸¡ì • (ê°œì„  ë²„ì „)."""

    if self.gpu_executor is None:
        return 0.0

    # 1. ë”ë¯¸ ìš”ì²­ ìƒì„±
    dummy_requests = create_dummy_requests(
        num_seqs=self.profile_batch_size,
        seq_len=self.profile_seq_len,
    )

    # 2. ì›Œë°ì—…
    for _ in range(2):
        self.gpu_executor.execute_model(dummy_requests)

    # 3. ì¸¡ì • (CUDA ë™ê¸°í™” í¬í•¨)
    torch.cuda.synchronize()
    start = time.perf_counter()

    for _ in range(num_batches):
        self.gpu_executor.execute_model(dummy_requests)

    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start

    # 4. ì²˜ë¦¬ëŸ‰ ê³„ì‚°
    total_tokens = num_batches * self.profile_seq_len * self.profile_batch_size
    throughput = total_tokens / elapsed

    return throughput
```

### 3.2.5 ê³ ê¸‰ í”„ë¡œíŒŒì¼ë§ (í–¥í›„ êµ¬í˜„)

```
ê³ ê¸‰ í”„ë¡œíŒŒì¼ë§ ì „ëµ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚  1. ë‹¤ì¤‘ ë°°ì¹˜ í¬ê¸° í”„ë¡œíŒŒì¼ë§                                    â”‚
â”‚     batch_sizes = [1, 4, 8, 16, 32]                             â”‚
â”‚     ê° ë°°ì¹˜ í¬ê¸°ì—ì„œ GPU/CPU ì²˜ë¦¬ëŸ‰ ì¸¡ì •                         â”‚
â”‚     â†’ ë°°ì¹˜ í¬ê¸°ë³„ ìµœì  ë¹„ìœ¨ í…Œì´ë¸” ìƒì„±                          â”‚
â”‚                                                                  â”‚
â”‚  2. ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ í”„ë¡œíŒŒì¼ë§                                     â”‚
â”‚     seq_lens = [128, 512, 1024, 2048]                           â”‚
â”‚     ê¸´ ì‹œí€€ìŠ¤ â†’ GPU ì„ í˜¸ (compute bound)                        â”‚
â”‚     ì§§ì€ ì‹œí€€ìŠ¤ â†’ CPU ê°€ëŠ¥ (memory bound)                       â”‚
â”‚                                                                  â”‚
â”‚  3. ë™ì  ë¹„ìœ¨ ì¡°ì •                                               â”‚
â”‚     ëŸ°íƒ€ì„ì— ì‹¤ì œ ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§                                â”‚
â”‚     í¸ì°¨ ë°œìƒ ì‹œ ë¹„ìœ¨ ì¬ì¡°ì •                                     â”‚
â”‚                                                                  â”‚
â”‚  4. ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ ê³ ë ¤                                         â”‚
â”‚     CPU ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰                                       â”‚
â”‚     GPU VRAM ì—¬ìœ  ê³µê°„                                          â”‚
â”‚     KV Cache í¬ê¸°                                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2.6 í”„ë¡œíŒŒì¼ë§ ì„¤ì • íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|--------|------|
| `profile_seq_len` | 128 | í”„ë¡œíŒŒì¼ë§ìš© ì‹œí€€ìŠ¤ ê¸¸ì´ |
| `profile_batch_size` | 1 | í”„ë¡œíŒŒì¼ë§ìš© ë°°ì¹˜ í¬ê¸° |
| `profile_num_batches` | 5 | ì¸¡ì • ë°˜ë³µ íšŸìˆ˜ |
| `warmup_iterations` | 2 | ì›Œë°ì—… ë°˜ë³µ íšŸìˆ˜ |

### 3.2.7 ìˆ˜ë™ ë¹„ìœ¨ ì§€ì • vs ìë™ í”„ë¡œíŒŒì¼ë§

```bash
# ìë™ í”„ë¡œíŒŒì¼ë§ (ê¸°ë³¸ê°’)
--hybrid-mode parallel-batch
# â†’ ì„œë²„ ì‹œì‘ ì‹œ CPU/GPU ì²˜ë¦¬ëŸ‰ ì¸¡ì • í›„ ë¹„ìœ¨ ìë™ ê²°ì •

# ìˆ˜ë™ ë¹„ìœ¨ ì§€ì • (í”„ë¡œíŒŒì¼ë§ ê±´ë„ˆëœ€)
--hybrid-mode parallel-batch --hybrid-cpu-ratio 0.1
# â†’ CPUê°€ ì „ì²´ ìš”ì²­ì˜ 10% ì²˜ë¦¬, í”„ë¡œíŒŒì¼ë§ ìƒëµ
```

**ê¶Œì¥ ì‚¬í•­:**
- ì²« ì‹¤í–‰: ìë™ í”„ë¡œíŒŒì¼ë§ìœ¼ë¡œ ìµœì  ë¹„ìœ¨ í™•ì¸
- ìš´ì˜ í™˜ê²½: í™•ì¸ëœ ë¹„ìœ¨ì„ ìˆ˜ë™ ì§€ì •í•˜ì—¬ ì‹œì‘ ì‹œê°„ ë‹¨ì¶•

#### 3.1.2 êµ¬í˜„

```python
# vllm/executor/apex_executor.py

class APEXProfiler:
    """CPU/GPU ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""

    def __init__(self, model_config: ModelConfig):
        self.model_config = model_config
        self.gpu_throughput: Optional[float] = None
        self.cpu_throughput: Optional[float] = None

    def profile(self) -> Tuple[float, float]:
        """CPU/GPU ì²˜ë¦¬ëŸ‰ ì¸¡ì •"""

        # GPU í”„ë¡œíŒŒì¼ë§
        gpu_model = load_model(self.model_config).cuda()
        self.gpu_throughput = self._measure_throughput(gpu_model, 'cuda')
        del gpu_model
        torch.cuda.empty_cache()

        # CPU í”„ë¡œíŒŒì¼ë§ (INT8 ì–‘ìí™”)
        cpu_model = load_model(self.model_config)
        cpu_model = quantize_to_int8(cpu_model).cpu()
        self.cpu_throughput = self._measure_throughput(cpu_model, 'cpu')
        del cpu_model

        return self.gpu_throughput, self.cpu_throughput

    def _measure_throughput(
        self,
        model: nn.Module,
        device: str,
        num_warmup: int = 3,
        num_measure: int = 10,
    ) -> float:
        """ì²˜ë¦¬ëŸ‰ ì¸¡ì • (tok/s)"""

        # ë”ë¯¸ ì…ë ¥
        input_ids = torch.randint(0, 32000, (1, 128)).to(device)

        # ì›Œë°ì—…
        for _ in range(num_warmup):
            with torch.no_grad():
                _ = model(input_ids)

        # ì¸¡ì •
        if device == 'cuda':
            torch.cuda.synchronize()

        start = time.perf_counter()
        for _ in range(num_measure):
            with torch.no_grad():
                _ = model(input_ids)

        if device == 'cuda':
            torch.cuda.synchronize()

        elapsed = time.perf_counter() - start

        # í† í°/ì´ˆ
        total_tokens = num_measure * 128
        return total_tokens / elapsed

    def get_optimal_ratio(self) -> Tuple[float, float]:
        """ìµœì  CPU/GPU ë°°ì¹˜ ë¹„ìœ¨"""

        if self.gpu_throughput is None:
            self.profile()

        total = self.gpu_throughput + self.cpu_throughput
        gpu_ratio = self.gpu_throughput / total
        cpu_ratio = self.cpu_throughput / total

        return gpu_ratio, cpu_ratio


class APEXScheduler:
    """APEX ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(
        self,
        gpu_ratio: float,
        cpu_ratio: float,
        max_batch_size: int = 64,
    ):
        self.gpu_ratio = gpu_ratio
        self.cpu_ratio = cpu_ratio
        self.max_batch_size = max_batch_size

    def partition_batch(
        self,
        requests: List[InferenceRequest],
    ) -> Tuple[List[InferenceRequest], List[InferenceRequest]]:
        """ë°°ì¹˜ë¥¼ GPU/CPUë¡œ ë¶„í• """

        n = len(requests)
        gpu_count = int(n * self.gpu_ratio)

        # GPUëŠ” ë” ê¸´ ìš”ì²­ (compute boundì—ì„œ íš¨ìœ¨ì )
        sorted_requests = sorted(requests, key=lambda r: len(r.input_ids), reverse=True)

        gpu_batch = sorted_requests[:gpu_count]
        cpu_batch = sorted_requests[gpu_count:]

        return gpu_batch, cpu_batch


class APEXExecutor:
    """APEX ì‹¤í–‰ê¸°"""

    def __init__(
        self,
        model_config: ModelConfig,
        cpu_config: CPUExecutorConfig,
    ):
        self.model_config = model_config

        # í”„ë¡œíŒŒì¼ë§
        profiler = APEXProfiler(model_config)
        gpu_ratio, cpu_ratio = profiler.get_optimal_ratio()

        logger.info(f"APEX optimal ratio - GPU: {gpu_ratio:.1%}, CPU: {cpu_ratio:.1%}")

        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = APEXScheduler(gpu_ratio, cpu_ratio)

        # GPU ì›Œì»¤
        self.gpu_worker = GPUWorker(model_config)

        # CPU ì›Œì»¤ (INT8 ì–‘ìí™”)
        self.cpu_worker = CPUWorkerINT8(model_config, cpu_config)

        # ê²°ê³¼ í
        self.result_queue = asyncio.Queue()

    async def execute_batch(
        self,
        requests: List[InferenceRequest],
    ) -> List[InferenceResponse]:
        """ë°°ì¹˜ ì‹¤í–‰ (CPU/GPU ë³‘ë ¬)"""

        # ë°°ì¹˜ ë¶„í• 
        gpu_batch, cpu_batch = self.scheduler.partition_batch(requests)

        # ë³‘ë ¬ ì‹¤í–‰
        gpu_task = asyncio.create_task(
            self.gpu_worker.execute(gpu_batch)
        ) if gpu_batch else None

        cpu_task = asyncio.create_task(
            self.cpu_worker.execute(cpu_batch)
        ) if cpu_batch else None

        # ê²°ê³¼ ìˆ˜ì§‘
        results = []

        if gpu_task:
            gpu_results = await gpu_task
            results.extend(gpu_results)

        if cpu_task:
            cpu_results = await cpu_task
            results.extend(cpu_results)

        # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬
        results.sort(key=lambda r: r.request_id)

        return results


class CPUWorkerINT8:
    """INT8 ì–‘ìí™”ëœ CPU ì›Œì»¤"""

    def __init__(
        self,
        model_config: ModelConfig,
        cpu_config: CPUExecutorConfig,
    ):
        # INT8 ëª¨ë¸ ë¡œë“œ
        self.model = load_model(model_config)
        self.model = quantize_to_int8_avx512(self.model)
        self.model = self.model.cpu()

        # ìŠ¤ë ˆë“œ ì„¤ì •
        torch.set_num_threads(cpu_config.num_threads)

        # AVX-512 ìµœì í™” í™˜ê²½ ì„¤ì •
        setup_avx512_environment()

    async def execute(
        self,
        requests: List[InferenceRequest],
    ) -> List[InferenceResponse]:
        """CPUì—ì„œ ë°°ì¹˜ ì‹¤í–‰"""

        # CPU ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰ (async ë¸”ë¡œí‚¹ ë°©ì§€)
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            None,  # default executor
            self._execute_sync,
            requests,
        )
        return results

    def _execute_sync(
        self,
        requests: List[InferenceRequest],
    ) -> List[InferenceResponse]:
        """ë™ê¸° ì‹¤í–‰"""

        responses = []

        for request in requests:
            input_ids = torch.tensor([request.input_ids])

            output_tokens = []
            with torch.no_grad():
                for _ in range(request.max_tokens):
                    logits = self.model(input_ids)
                    next_token = logits[:, -1, :].argmax(dim=-1).item()
                    output_tokens.append(next_token)
                    input_ids = torch.cat([
                        input_ids,
                        torch.tensor([[next_token]])
                    ], dim=1)

                    if next_token == self.eos_token_id:
                        break

            responses.append(InferenceResponse(
                request_id=request.id,
                output_tokens=output_tokens,
            ))

        return responses
```

---

## 4. ê³µí†µ ì¸í”„ë¼

### 4.1 ì„¤ì • í´ë˜ìŠ¤

```python
# vllm/config.py í™•ì¥

@dataclass
class HybridConfig:
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì„¤ì •"""

    # ëª¨ë“œ ì„ íƒ
    mode: Literal["option-a", "option-b", "none"] = "none"

    # Option A ì„¤ì •
    moe_offload: Optional[ExpertOffloadConfig] = None
    ngram_spec: Optional[NGramSpecConfig] = None
    disaggregated: Optional[DisaggregatedConfig] = None

    # Option B ì„¤ì •
    apex: Optional[APEXConfig] = None

    def validate(self):
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        if self.mode == "option-a":
            assert self.apex is None, "Option Aì—ì„œëŠ” APEX ì‚¬ìš© ë¶ˆê°€"
        elif self.mode == "option-b":
            assert self.moe_offload is None, "Option Bì—ì„œëŠ” MoE Offload ì‚¬ìš© ë¶ˆê°€"
            assert self.ngram_spec is None, "Option Bì—ì„œëŠ” N-gram Spec ì‚¬ìš© ë¶ˆê°€"
            assert self.disaggregated is None, "Option Bì—ì„œëŠ” Disaggregated ì‚¬ìš© ë¶ˆê°€"


@dataclass
class ExpertOffloadConfig:
    """MoE Expert ì˜¤í”„ë¡œë“œ ì„¤ì •"""
    enabled: bool = True
    num_gpu_experts: int = 8  # GPUì— ìƒì£¼í•  expert ìˆ˜
    cpu_dtype: str = "int8"   # CPU expert ë°ì´í„° íƒ€ì…
    swap_threshold: int = 100  # expert êµì²´ ì£¼ê¸°


@dataclass
class NGramSpecConfig:
    """N-gram Speculative Decoding ì„¤ì •"""
    enabled: bool = True
    n: int = 3                      # N-gram í¬ê¸°
    num_speculative_tokens: int = 5  # ì¶”ì¸¡ í† í° ìˆ˜
    min_frequency: int = 2           # ìµœì†Œ ì¶œí˜„ ë¹ˆë„


@dataclass
class DisaggregatedConfig:
    """Disaggregated Serving ì„¤ì •"""
    enabled: bool = True
    prefill_device: str = "gpu"      # "gpu" ë˜ëŠ” "cpu"
    num_prefill_nodes: int = 1
    num_decode_nodes: int = 1
    kv_transfer_method: str = "rdma"  # "rdma", "tcp", "shm"


@dataclass
class APEXConfig:
    """APEX ìŠ¤ì¼€ì¤„ë§ ì„¤ì •"""
    enabled: bool = True
    auto_profile: bool = True        # ìë™ í”„ë¡œíŒŒì¼ë§
    gpu_ratio: Optional[float] = None  # ìˆ˜ë™ ì„¤ì • ì‹œ
    cpu_ratio: Optional[float] = None
    cpu_dtype: str = "int8"
    cpu_num_threads: int = 48
```

### 4.2 CLI ì¸í„°í˜ì´ìŠ¤

```python
# vllm/entrypoints/openai/cli_args.py í™•ì¥

def add_hybrid_args(parser: argparse.ArgumentParser):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì¸ì ì¶”ê°€"""

    group = parser.add_argument_group("Hybrid Execution Options")

    # ëª¨ë“œ ì„ íƒ
    group.add_argument(
        "--hybrid-mode",
        type=str,
        choices=["option-a", "option-b", "none"],
        default="none",
        help="í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ëª¨ë“œ ì„ íƒ"
    )

    # Option A: MoE Offload
    group.add_argument(
        "--moe-cpu-offload",
        action="store_true",
        help="MoE Expertë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œ"
    )
    group.add_argument(
        "--moe-num-gpu-experts",
        type=int,
        default=8,
        help="GPUì— ìƒì£¼í•  expert ìˆ˜"
    )

    # Option A: N-gram Speculative
    group.add_argument(
        "--ngram-spec-decode",
        action="store_true",
        help="N-gram ê¸°ë°˜ Speculative Decoding í™œì„±í™”"
    )
    group.add_argument(
        "--ngram-n",
        type=int,
        default=3,
        help="N-gram í¬ê¸°"
    )
    group.add_argument(
        "--ngram-num-spec-tokens",
        type=int,
        default=5,
        help="ì¶”ì¸¡í•  í† í° ìˆ˜"
    )

    # Option A: Disaggregated
    group.add_argument(
        "--disaggregated-prefill",
        action="store_true",
        help="Prefill/Decode ë¶„ë¦¬ í™œì„±í™”"
    )
    group.add_argument(
        "--prefill-device",
        type=str,
        choices=["gpu", "cpu"],
        default="gpu",
        help="Prefill ì‹¤í–‰ ë””ë°”ì´ìŠ¤"
    )

    # Option B: APEX
    group.add_argument(
        "--apex-cpu-ratio",
        type=float,
        default=None,
        help="APEX CPU ë°°ì¹˜ ë¹„ìœ¨ (ìë™ í”„ë¡œíŒŒì¼ë§ ì‹œ ìƒëµ)"
    )
    group.add_argument(
        "--apex-cpu-threads",
        type=int,
        default=48,
        help="APEX CPU ìŠ¤ë ˆë“œ ìˆ˜"
    )

    return parser
```

### 4.3 ì‹¤í–‰ ì˜ˆì‹œ

```bash
#!/bin/bash

# =============================================================================
# Option A: MoE Offload + N-gram + Disaggregated (DeepSeek R1 ê¶Œì¥)
# =============================================================================

# ì „ì²´ Option A í™œì„±í™”
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --hybrid-mode option-a \
  --moe-cpu-offload \
  --moe-num-gpu-experts 8 \
  --ngram-spec-decode \
  --ngram-n 3 \
  --ngram-num-spec-tokens 5 \
  --disaggregated-prefill \
  --prefill-device gpu \
  --tensor-parallel-size 8

# MoE Offloadë§Œ (ê°„ë‹¨ ë²„ì „)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --hybrid-mode option-a \
  --moe-cpu-offload \
  --tensor-parallel-size 8

# N-gramë§Œ ì¶”ê°€
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --hybrid-mode option-a \
  --ngram-spec-decode \
  --tensor-parallel-size 8

# =============================================================================
# Option B: APEX ìŠ¤ì¼€ì¤„ë§ (Dense ëª¨ë¸ ë˜ëŠ” ì œí•œëœ GPU)
# =============================================================================

# ìë™ í”„ë¡œíŒŒì¼ë§
vllm serve meta-llama/Llama-3-70B \
  --hybrid-mode option-b \
  --apex-cpu-threads 48 \
  --tensor-parallel-size 8

# ìˆ˜ë™ ë¹„ìœ¨ ì§€ì •
vllm serve meta-llama/Llama-3-70B \
  --hybrid-mode option-b \
  --apex-cpu-ratio 0.2 \
  --apex-cpu-threads 48 \
  --tensor-parallel-size 8

# =============================================================================
# ë¹„êµ ë²¤ì¹˜ë§ˆí¬
# =============================================================================

# GPU only (ê¸°ì¤€)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --tensor-parallel-size 8

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python benchmarks/benchmark_serving.py \
  --backend openai \
  --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --num-prompts 500 \
  --random-input-len 128 \
  --random-output-len 128
```

---

## 5. êµ¬í˜„ ë¡œë“œë§µ

### 5.1 Phase 1: ê¸°ë°˜ ì¸í”„ë¼ (1ì£¼)

```
Week 1:
â”œâ”€â”€ Day 1-2: ì„¤ì • í´ë˜ìŠ¤ ë° CLI ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ HybridConfig, ExpertOffloadConfig ë“±
â”‚   â””â”€â”€ argparse í™•ì¥
â”‚
â”œâ”€â”€ Day 3-4: CPU INT8 ì–‘ìí™” íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ AVX-512 VNNI ì»¤ë„ (Phase 1ì—ì„œ êµ¬í˜„í•œ ê²ƒ í™œìš©)
â”‚   â””â”€â”€ ëª¨ë¸ ì–‘ìí™” í•¨ìˆ˜
â”‚
â””â”€â”€ Day 5-7: í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
    â”œâ”€â”€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
    â””â”€â”€ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ì¼ˆë ˆí†¤
```

### 5.2 Phase 2: Option A - MoE Offload (1.5ì£¼)

```
Week 2-3:
â”œâ”€â”€ Day 1-3: ExpertOffloadManager êµ¬í˜„
â”‚   â”œâ”€â”€ GPU/CPU expert ë¶„ë¦¬
â”‚   â”œâ”€â”€ LRU ìºì‹œ ë¡œì§
â”‚   â””â”€â”€ ë¹„ë™ê¸° ì „ì†¡
â”‚
â”œâ”€â”€ Day 4-6: FusedMoEWithOffload êµ¬í˜„
â”‚   â”œâ”€â”€ ê¸°ì¡´ MoE ë ˆì´ì–´ ë˜í•‘
â”‚   â”œâ”€â”€ ë¼ìš°íŒ… + ì˜¤í”„ë¡œë“œ í†µí•©
â”‚   â””â”€â”€ CPU expert INT8 ì‹¤í–‰
â”‚
â””â”€â”€ Day 7-10: í†µí•© ë° í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ DeepSeek R1 ëª¨ë¸ í…ŒìŠ¤íŠ¸
    â””â”€â”€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```

### 5.3 Phase 3: Option A - N-gram Lookahead (1ì£¼)

```
Week 4:
â”œâ”€â”€ Day 1-2: NGramProposer êµ¬í˜„
â”‚   â”œâ”€â”€ N-gram ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ íŒ¨í„´ ë§¤ì¹­ ë¡œì§
â”‚   â””â”€â”€ ì—…ë°ì´íŠ¸ ë¡œì§
â”‚
â”œâ”€â”€ Day 3-4: vLLM Speculative Decoding í†µí•©
â”‚   â”œâ”€â”€ NGramSpeculativeWorker
â”‚   â””â”€â”€ ê²€ì¦ ë¡œì§
â”‚
â””â”€â”€ Day 5-7: í…ŒìŠ¤íŠ¸ ë° íŠœë‹
    â”œâ”€â”€ ì½”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬
    â””â”€â”€ ì±„íƒë¥  ë¶„ì„
```

### 5.4 Phase 4: Option A - Disaggregated (1.5ì£¼)

```
Week 5-6:
â”œâ”€â”€ Day 1-3: KV Cache ì „ì†¡ ì¸í”„ë¼
â”‚   â”œâ”€â”€ KVCacheSender / KVCacheReceiver
â”‚   â”œâ”€â”€ TCP / RDMA / SHM ë°±ì—”ë“œ
â”‚   â””â”€â”€ ë¹„ë™ê¸° ì „ì†¡
â”‚
â”œâ”€â”€ Day 4-6: PrefillNode / DecodeNode êµ¬í˜„
â”‚   â”œâ”€â”€ Prefill ìµœì í™” ëª¨ë¸ ë¡œë“œ
â”‚   â”œâ”€â”€ Decode + MoE Offload + N-gram í†µí•©
â”‚   â””â”€â”€ ì¡°ìœ¨ì (Coordinator)
â”‚
â””â”€â”€ Day 7-10: ë©€í‹°ë…¸ë“œ í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ ë‹¨ì¼ ë¨¸ì‹  í…ŒìŠ¤íŠ¸
    â””â”€â”€ ë©€í‹° ë¨¸ì‹  í…ŒìŠ¤íŠ¸ (ì„ íƒì )
```

### 5.5 Phase 5: Option B - APEX (1ì£¼)

```
Week 7:
â”œâ”€â”€ Day 1-2: APEXProfiler êµ¬í˜„
â”‚   â”œâ”€â”€ GPU ì²˜ë¦¬ëŸ‰ ì¸¡ì •
â”‚   â”œâ”€â”€ CPU ì²˜ë¦¬ëŸ‰ ì¸¡ì •
â”‚   â””â”€â”€ ìµœì  ë¹„ìœ¨ ê³„ì‚°
â”‚
â”œâ”€â”€ Day 3-4: APEXScheduler + APEXExecutor êµ¬í˜„
â”‚   â”œâ”€â”€ ë°°ì¹˜ ë¶„í•  ë¡œì§
â”‚   â”œâ”€â”€ GPU/CPU ë³‘ë ¬ ì‹¤í–‰
â”‚   â””â”€â”€ ê²°ê³¼ ë³‘í•©
â”‚
â””â”€â”€ Day 5-7: í…ŒìŠ¤íŠ¸ ë° íŠœë‹
    â”œâ”€â”€ Dense ëª¨ë¸ (Llama) í…ŒìŠ¤íŠ¸
    â””â”€â”€ ë‹¤ì–‘í•œ GPU í™˜ê²½ í…ŒìŠ¤íŠ¸
```

### 5.6 Phase 6: í†µí•© ë° ìµœì í™” (1ì£¼)

```
Week 8:
â”œâ”€â”€ Day 1-3: ì „ì²´ í†µí•©
â”‚   â”œâ”€â”€ Option A/B ìŠ¤ìœ„ì¹­ ë¡œì§
â”‚   â”œâ”€â”€ ì—ëŸ¬ ì²˜ë¦¬
â”‚   â””â”€â”€ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
â”‚
â”œâ”€â”€ Day 4-5: ì„±ëŠ¥ ìµœì í™”
â”‚   â”œâ”€â”€ ë³‘ëª© ë¶„ì„
â”‚   â”œâ”€â”€ ë©”ëª¨ë¦¬ ìµœì í™”
â”‚   â””â”€â”€ ì§€ì—°ì‹œê°„ ìµœì í™”
â”‚
â””â”€â”€ Day 6-7: ë¬¸ì„œí™” ë° ë¦´ë¦¬ìŠ¤
    â”œâ”€â”€ ì‚¬ìš©ì ê°€ì´ë“œ
    â”œâ”€â”€ API ë¬¸ì„œ
    â””â”€â”€ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
```

---

## 6. íŒŒì¼ êµ¬ì¡°

```
vllm/
â”œâ”€â”€ config.py                          # [ìˆ˜ì •] HybridConfig ì¶”ê°€
â”œâ”€â”€ entrypoints/
â”‚   â””â”€â”€ openai/
â”‚       â””â”€â”€ cli_args.py                # [ìˆ˜ì •] í•˜ì´ë¸Œë¦¬ë“œ ì¸ì ì¶”ê°€
â”‚
â”œâ”€â”€ executor/
â”‚   â”œâ”€â”€ hybrid_executor.py             # [ì‹ ê·œ] í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ê¸° íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ apex_executor.py               # [ì‹ ê·œ] Option B: APEX
â”‚   â””â”€â”€ disaggregated_executor.py      # [ì‹ ê·œ] Option A: Disaggregated
â”‚
â”œâ”€â”€ model_executor/
â”‚   â””â”€â”€ layers/
â”‚       â””â”€â”€ moe/
â”‚           â”œâ”€â”€ expert_offload.py      # [ì‹ ê·œ] Expert ì˜¤í”„ë¡œë“œ ê´€ë¦¬ì
â”‚           â””â”€â”€ fused_moe_offload.py   # [ì‹ ê·œ] ì˜¤í”„ë¡œë“œ MoE ë ˆì´ì–´
â”‚
â”œâ”€â”€ spec_decode/
â”‚   â”œâ”€â”€ ngram_proposer.py              # [ì‹ ê·œ] N-gram ì œì•ˆì
â”‚   â””â”€â”€ ngram_spec_worker.py           # [ì‹ ê·œ] N-gram Spec ì›Œì»¤
â”‚
â”œâ”€â”€ engine/
â”‚   â””â”€â”€ disaggregated/
â”‚       â”œâ”€â”€ prefill_node.py            # [ì‹ ê·œ] Prefill ë…¸ë“œ
â”‚       â”œâ”€â”€ decode_node.py             # [ì‹ ê·œ] Decode ë…¸ë“œ
â”‚       â”œâ”€â”€ coordinator.py             # [ì‹ ê·œ] ì¡°ìœ¨ì
â”‚       â””â”€â”€ kv_transfer.py             # [ì‹ ê·œ] KV ìºì‹œ ì „ì†¡
â”‚
â”œâ”€â”€ worker/
â”‚   â””â”€â”€ cpu_worker_int8.py             # [ì‹ ê·œ] INT8 CPU ì›Œì»¤
â”‚
â””â”€â”€ platforms/
    â””â”€â”€ intel_cpu_utils.py             # [ìˆ˜ì •] AVX-512 VNNI ìµœì í™” ì¶”ê°€

csrc/cpu/
â”œâ”€â”€ gemm_vnni.cpp                      # [ì‹ ê·œ] VNNI GEMM (ì´ì „ ê³„íšì—ì„œ)
â”œâ”€â”€ expert_compute.cpp                 # [ì‹ ê·œ] Expert CPU ê³„ì‚° ì»¤ë„
â””â”€â”€ torch_bindings.cpp                 # [ìˆ˜ì •] ë°”ì¸ë”© ì¶”ê°€

tests/
â”œâ”€â”€ hybrid/
â”‚   â”œâ”€â”€ test_moe_offload.py            # [ì‹ ê·œ]
â”‚   â”œâ”€â”€ test_ngram_proposer.py         # [ì‹ ê·œ]
â”‚   â”œâ”€â”€ test_disaggregated.py          # [ì‹ ê·œ]
â”‚   â””â”€â”€ test_apex.py                   # [ì‹ ê·œ]
â””â”€â”€ benchmarks/
    â””â”€â”€ bench_hybrid_options.py        # [ì‹ ê·œ]

docs/
â”œâ”€â”€ HYBRID_OPTIONS_IMPLEMENTATION_PLAN.md  # ì´ ë¬¸ì„œ
â””â”€â”€ HYBRID_USER_GUIDE.md               # [ì‹ ê·œ] ì‚¬ìš©ì ê°€ì´ë“œ
```

---

## 7. API ì„¤ê³„

### 7.1 Python API

```python
from vllm import LLM, HybridConfig, ExpertOffloadConfig, NGramSpecConfig

# Option A: MoE ìµœì í™”
llm = LLM(
    model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    hybrid_config=HybridConfig(
        mode="option-a",
        moe_offload=ExpertOffloadConfig(
            enabled=True,
            num_gpu_experts=8,
        ),
        ngram_spec=NGramSpecConfig(
            enabled=True,
            n=3,
            num_speculative_tokens=5,
        ),
    ),
    tensor_parallel_size=8,
)

# Option B: APEX
llm = LLM(
    model="meta-llama/Llama-3-70B",
    hybrid_config=HybridConfig(
        mode="option-b",
        apex=APEXConfig(
            enabled=True,
            auto_profile=True,
        ),
    ),
    tensor_parallel_size=8,
)

# ì¶”ë¡ 
outputs = llm.generate(prompts, sampling_params)
```

### 7.2 REST API í™•ì¥

```
# ìƒíƒœ í™•ì¸
GET /v1/hybrid/status
Response:
{
  "mode": "option-a",
  "components": {
    "moe_offload": {
      "enabled": true,
      "gpu_experts": 8,
      "cpu_experts": 248
    },
    "ngram_spec": {
      "enabled": true,
      "acceptance_rate": 0.72
    },
    "disaggregated": {
      "enabled": true,
      "prefill_nodes": 1,
      "decode_nodes": 1
    }
  }
}

# ëŸ°íƒ€ì„ ì„¤ì • ë³€ê²½ (ì„ íƒì )
POST /v1/hybrid/config
{
  "ngram_num_speculative_tokens": 7
}
```

---

## 8. í…ŒìŠ¤íŠ¸ ê³„íš

### 8.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/hybrid/test_moe_offload.py

class TestExpertOffloadManager:

    def test_expert_routing(self):
        """Expert ë¼ìš°íŒ… ì •í™•ì„±"""
        manager = ExpertOffloadManager(num_experts=256, num_gpu_experts=8)

        hidden = torch.randn(32, 4096)
        router_logits = torch.randn(32, 256)

        output = manager.route_and_compute(hidden, router_logits)

        assert output.shape == hidden.shape

    def test_cpu_expert_int8(self):
        """CPU Expert INT8 ê³„ì‚° ì •í™•ì„±"""
        expert = MoEExpert(4096, 11008)
        expert_int8 = quantize_expert_to_int8(expert)

        input_tensor = torch.randn(32, 4096)

        output_fp32 = expert(input_tensor)
        output_int8 = expert_int8.int8_forward(input_tensor)

        # ìƒëŒ€ ì˜¤ì°¨ < 1%
        rel_error = (output_fp32 - output_int8).abs() / output_fp32.abs().clamp(min=1e-5)
        assert rel_error.mean() < 0.01

    def test_expert_swap(self):
        """Expert LRU êµì²´"""
        manager = ExpertOffloadManager(num_experts=16, num_gpu_experts=4)

        # Expert 0-3ì„ ë§ì´ ì‚¬ìš©
        for _ in range(100):
            manager.expert_usage_count[0] += 1
            manager.expert_usage_count[1] += 1

        manager._swap_experts()

        # 0, 1ì´ GPUì— ìˆì–´ì•¼ í•¨
        assert 0 in manager.gpu_experts
        assert 1 in manager.gpu_experts


# tests/hybrid/test_ngram_proposer.py

class TestNGramProposer:

    def test_ngram_update(self):
        """N-gram í…Œì´ë¸” ì—…ë°ì´íŠ¸"""
        proposer = NGramProposer(n=3)

        # "The quick brown fox" í•™ìŠµ
        tokens = [100, 200, 300, 400]  # The, quick, brown, fox
        proposer.update(tokens)

        # (quick, brown) â†’ fox ë§¤í•‘ í™•ì¸
        assert (200, 300) in proposer.ngram_store
        assert 400 in proposer.ngram_store[(200, 300)]

    def test_ngram_propose(self):
        """N-gram ì¶”ì¸¡"""
        proposer = NGramProposer(n=3, min_frequency=1)

        # í•™ìŠµ
        proposer.update([100, 200, 300, 400, 500])
        proposer.update([100, 200, 300, 400, 500])  # ë¹ˆë„ 2

        # ì¶”ì¸¡
        proposals, _ = proposer.propose([100, 200, 300])

        assert proposals[0] == 400  # fox


# tests/hybrid/test_apex.py

class TestAPEXScheduler:

    def test_batch_partition(self):
        """ë°°ì¹˜ ë¶„í•  í…ŒìŠ¤íŠ¸"""
        scheduler = APEXScheduler(gpu_ratio=0.8, cpu_ratio=0.2)

        requests = [InferenceRequest(id=i) for i in range(10)]

        gpu_batch, cpu_batch = scheduler.partition_batch(requests)

        assert len(gpu_batch) == 8
        assert len(cpu_batch) == 2
```

### 8.2 í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/hybrid/test_integration.py

class TestOptionAIntegration:

    @pytest.mark.skipif(not is_moe_model_available(), reason="MoE ëª¨ë¸ í•„ìš”")
    def test_full_option_a(self):
        """Option A ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸"""

        llm = LLM(
            model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
            hybrid_config=HybridConfig(
                mode="option-a",
                moe_offload=ExpertOffloadConfig(enabled=True),
                ngram_spec=NGramSpecConfig(enabled=True),
            ),
            tensor_parallel_size=8,
        )

        outputs = llm.generate(["Hello, world!"], SamplingParams(max_tokens=50))

        assert len(outputs) == 1
        assert len(outputs[0].outputs[0].text) > 0


class TestOptionBIntegration:

    def test_full_option_b(self):
        """Option B ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸"""

        llm = LLM(
            model="meta-llama/Llama-3-8B",  # ì‘ì€ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
            hybrid_config=HybridConfig(
                mode="option-b",
                apex=APEXConfig(enabled=True),
            ),
        )

        outputs = llm.generate(["Hello, world!"], SamplingParams(max_tokens=50))

        assert len(outputs) == 1
```

### 8.3 ë²¤ì¹˜ë§ˆí¬

```python
# tests/benchmarks/bench_hybrid_options.py

def benchmark_all_modes():
    """ëª¨ë“  ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬"""

    results = {}

    # GPU only (ê¸°ì¤€)
    results["gpu_only"] = benchmark_mode(
        model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        hybrid_mode="none",
    )

    # Option A: Full
    results["option_a_full"] = benchmark_mode(
        model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        hybrid_mode="option-a",
        moe_offload=True,
        ngram_spec=True,
    )

    # Option A: MoE only
    results["option_a_moe"] = benchmark_mode(
        model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        hybrid_mode="option-a",
        moe_offload=True,
        ngram_spec=False,
    )

    # Option B
    results["option_b"] = benchmark_mode(
        model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        hybrid_mode="option-b",
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("Benchmark Results")
    print("="*60)

    for mode, result in results.items():
        print(f"\n{mode}:")
        print(f"  Throughput: {result['throughput']:.2f} tok/s")
        print(f"  TTFT: {result['ttft']:.2f} ms")
        print(f"  TPOT: {result['tpot']:.2f} ms")
        print(f"  vs GPU only: {result['throughput']/results['gpu_only']['throughput']:.2f}x")
```

---

## 9. ì˜ˆìƒ ì„±ëŠ¥

### 9.1 Option A (DeepSeek R1 70B)

| êµ¬ì„± | ì²˜ë¦¬ëŸ‰ (tok/s) | vs GPU only |
|------|----------------|-------------|
| GPU only | 5,742 | 1.0x |
| + MoE Offload | 7,000-8,000 | 1.2-1.4x |
| + N-gram Spec | 8,500-10,000 | 1.5-1.7x |
| + Disaggregated | 10,000-15,000 | 1.7-2.6x |

### 9.2 Option B (Llama 70B)

| êµ¬ì„± | ì²˜ë¦¬ëŸ‰ (tok/s) | vs GPU only |
|------|----------------|-------------|
| GPU only | 5,742 | 1.0x |
| APEX (H100) | 6,000-6,500 | 1.05-1.13x |
| APEX (T4) | 1.8-2.0x baseline | - |

---

## 10. ê²°ë¡ 

### Option A vs Option B ì„ íƒ ê°€ì´ë“œ

| ì¡°ê±´ | ê¶Œì¥ ì˜µì…˜ |
|------|----------|
| MoE ëª¨ë¸ (DeepSeek, Mixtral) | **Option A** |
| Dense ëª¨ë¸ (Llama, GPT) | Option B ë˜ëŠ” GPU only |
| GPU ë©”ëª¨ë¦¬ ë¶€ì¡± | **Option A** (MoE Offload) |
| ì œí•œëœ GPU (T4, A10) | **Option B** |
| H100 ì¶©ë¶„ | GPU only (ë˜ëŠ” Option A) |
| ìµœëŒ€ ì²˜ë¦¬ëŸ‰ ëª©í‘œ | **Option A** (ì „ì²´ í™œì„±í™”) |

### í•µì‹¬ ì„±ê³µ ìš”ì¸

1. **MoE Offload**: GPU ë©”ëª¨ë¦¬ ì ˆì•½ â†’ ë” í° ë°°ì¹˜ â†’ ì²˜ë¦¬ëŸ‰ ì¦ê°€
2. **N-gram**: CPUì˜ "ê³µì§œ" ì¶”ì¸¡ â†’ GPU íš¨ìœ¨ ì¦ê°€
3. **Disaggregated**: Prefill/Decode ë¶„ë¦¬ â†’ ê°ê° ìµœì í™”

---

*ì‘ì„±ì¼: 2026-02-03*
*ë²„ì „: 1.0*
